#!/usr/bin/env python3
"""
AIç§‘æŠ€èµ„è®¯æ™ºèƒ½åˆ†æç³»ç»Ÿ
æŠ“å–è¿‡å»24å°æ—¶AI/ç§‘æŠ€èµ„è®¯ï¼Œè¿›è¡Œæ™ºèƒ½åˆ†æï¼Œç”Ÿæˆæ·±åº¦æŠ¥å‘Šå¹¶æ¨é€
"""

import os
import re
import json
import requests
import hashlib
from datetime import datetime, timedelta
import time
from bs4 import BeautifulSoup
import feedparser
from urllib.parse import urljoin
from collections import Counter

class AITechNewsAnalyzer:
    def __init__(self):
        self.server_chan_key = os.getenv('SERVER_CHAN_KEY')
        self.zhipu_api_key = os.getenv('ZHIPU_API_KEY')
        self.twenty_four_hours_ago = datetime.now() - timedelta(hours=24)
        
        # é…ç½®å¤šä¸ªAI/ç§‘æŠ€æ–°é—»æº
        self.news_sources = [
            # å›½é™…AIç ”ç©¶
            {
                'name': 'Arxiv AI Papers',
                'url': 'http://arxiv.org/list/cs.AI/recent',
                'type': 'arxiv',
                'category': 'research'
            },
            {
                'name': 'MIT Tech Review AI',
                'url': 'https://www.technologyreview.com/topic/artificial-intelligence/feed/',
                'type': 'rss',
                'category': 'research'
            },
            # å›½é™…ç§‘æŠ€åª’ä½“
            {
                'name': 'TechCrunch AI',
                'url': 'https://techcrunch.com/category/artificial-intelligence/feed/',
                'type': 'rss',
                'category': 'tech'
            },
            {
                'name': 'VentureBeat AI',
                'url': 'https://venturebeat.com/category/ai/feed/',
                'type': 'rss',
                'category': 'tech'
            },
            {
                'name': 'The Verge AI',
                'url': 'https://www.theverge.com/ai-artificial-intelligence/rss',
                'type': 'rss',
                'category': 'tech'
            },
            # å¼€å‘è€…ç¤¾åŒº
            {
                'name': 'Hacker News AI',
                'url': 'https://hn.algolia.com/api/v1/search_by_date?tags=story&numericFilters=created_at_i>{}&query=AI',
                'type': 'hn_api',
                'category': 'community'
            },
            # ä¸­æ–‡AIåª’ä½“
            {
                'name': 'æœºå™¨ä¹‹å¿ƒ',
                'url': 'https://www.jiqizhixin.com/feed',
                'type': 'rss',
                'category': 'cn_ai'
            },
            {
                'name': 'é‡å­ä½',
                'url': 'https://www.qbitai.com/feed',
                'type': 'rss',
                'category': 'cn_ai'
            },
            {
                'name': 'AIç§‘æŠ€è¯„è®º',
                'url': 'https://www.leiphone.com/feed',
                'type': 'rss',
                'category': 'cn_ai'
            }
        ]
        
        self.all_articles = []
        self.ai_articles = []
        self.deep_analyses = []
        self.featured_article = None
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
    
    def fetch_arxiv(self, source):
        """æŠ“å–Arxiv AIè®ºæ–‡"""
        try:
            response = requests.get(source['url'], headers=self.headers, timeout=15)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                dt_list = soup.find_all('dt')
                dd_list = soup.find_all('dd')
                
                for i, (dt, dd) in enumerate(zip(dt_list[:8], dd_list[:8])):
                    paper_id_elem = dt.find('a', title='Abstract')
                    if not paper_id_elem:
                        continue
                    
                    paper_id = paper_id_elem.text.strip()
                    title_elem = dd.find('div', class_='list-title')
                    authors_elem = dd.find('div', class_='list-authors')
                    abstract_elem = dd.find('p')
                    
                    if title_elem:
                        title = title_elem.text.replace('Title:', '').strip()
                        authors = authors_elem.text.replace('Authors:', '').strip() if authors_elem else ''
                        abstract = abstract_elem.text.strip() if abstract_elem else ''
                        
                        article = {
                            'id': f"arxiv_{paper_id}",
                            'title': f"[è®ºæ–‡] {title[:120]}",
                            'link': f'https://arxiv.org/abs/{paper_id}',
                            'source': source['name'],
                            'summary': abstract[:200] + '...' if len(abstract) > 200 else abstract,
                            'authors': authors,
                            'category': 'research',
                            'importance': 9,
                            'time': datetime.now().strftime('%Y-%m-%d'),
                            'full_text': f"æ ‡é¢˜: {title}\nä½œè€…: {authors}\næ‘˜è¦: {abstract}"
                        }
                        self.all_articles.append(article)
                        self.ai_articles.append(article)
        except Exception as e:
            print(f"âš ï¸ Arxivè®ºæ–‡æŠ“å–å¤±è´¥: {e}")
    
    def fetch_rss(self, source):
        """æŠ“å–RSSæº"""
        try:
            feed = feedparser.parse(source['url'])
            for entry in feed.entries[:10]:
                # æ£€æŸ¥å‘å¸ƒæ—¶é—´
                pub_time = None
                if hasattr(entry, 'published_parsed'):
                    pub_time = datetime(*entry.published_parsed[:6])
                elif hasattr(entry, 'updated_parsed'):
                    pub_time = datetime(*entry.updated_parsed[:6])
                
                if pub_time and pub_time < self.twenty_four_hours_ago:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦AIç›¸å…³
                title = entry.get('title', '')
                summary = entry.get('summary', '')
                content = f"{title} {summary}".lower()
                
                ai_keywords = ['ai', 'artificial intelligence', 'machine learning', 
                              'deep learning', 'neural network', 'llm', 'gpt', 'transformer',
                              'äººå·¥æ™ºèƒ½', 'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'å¤§æ¨¡å‹','ç”Ÿæˆå¼AI','è®¡ç®—æœºè§†è§‰','å›¾åƒç”Ÿæˆ','è®­ç»ƒ'ï¼Œ
                              'AIGC','Diffusionæ¨¡å‹','MoEæ¨¡å‹','RLHF']
                
                is_ai_related = any(keyword in content for keyword in ai_keywords)
                
                article = {
                    'id': hashlib.md5(entry.get('link', '').encode()).hexdigest()[:8],
                    'title': title[:150],
                    'link': entry.get('link', ''),
                    'source': source['name'],
                    'summary': summary[:250] + '...' if len(summary) > 250 else summary,
                    'category': source['category'],
                    'importance': 8 if is_ai_related else 6,
                    'time': pub_time.strftime('%Y-%m-%d %H:%M') if pub_time else 'æœªçŸ¥',
                    'full_text': f"æ ‡é¢˜: {title}\næ‘˜è¦: {summary}"
                }
                
                self.all_articles.append(article)
                if is_ai_related:
                    self.ai_articles.append(article)
                    
        except Exception as e:
            print(f"âš ï¸ RSSç½‘ç«™æŠ“å–å¤±è´¥ {source['name']}: {e}")
    
    def fetch_hackernews(self, source):
        """æŠ“å–Hacker News AIå†…å®¹"""
        try:
            timestamp = int(self.twenty_four_hours_ago.timestamp())
            url = source['url'].format(timestamp)
            response = requests.get(url, timeout=10)
            
            if response.status_code == 200:
                hits = response.json().get('hits', [])
                for hit in hits[:15]:
                    title = hit.get('title', '').lower()
                    if not any(keyword in title for keyword in ['ai', 'llm', 'gpt', 'openai', 'anthropic']):
                        continue
                    
                    article = {
                        'id': f"hn_{hit.get('objectID', '')}",
                        'title': hit.get('title', ''),
                        'link': hit.get('url', f"https://news.ycombinator.com/item?id={hit.get('objectID')}"),
                        'source': source['name'],
                        'points': hit.get('points', 0),
                        'comments': hit.get('num_comments', 0),
                        'category': 'community',
                        'importance': min(9, 7 + (hit.get('points', 0) // 20)),
                        'time': datetime.fromtimestamp(hit.get('created_at_i', 0)).strftime('%Y-%m-%d %H:%M'),
                        'full_text': f"æ ‡é¢˜: {hit.get('title', '')}\nå¾—åˆ†: {hit.get('points', 0)} | è¯„è®º: {hit.get('num_comments', 0)}"
                    }
                    self.all_articles.append(article)
                    self.ai_articles.append(article)
                    
        except Exception as e:
            print(f"âš ï¸ Hacker NewsæŠ“å–å¤±è´¥: {e}")
    
    def fetch_all_news(self):
        """æŠ“å–æ‰€æœ‰æ–°é—»æº"""
        print("ğŸ“¡ å¼€å§‹æŠ“å–æ–°é—»æº...")
        for source in self.news_sources:
            print(f"  â†’ {source['name']}")
            try:
                if source['type'] == 'arxiv':
                    self.fetch_arxiv(source)
                elif source['type'] == 'rss':
                    self.fetch_rss(source)
                elif source['type'] == 'hn_api':
                    self.fetch_hackernews(source)
                time.sleep(1)  # ç¤¼è²Œå»¶è¿Ÿ
            except Exception as e:
                print(f"    âŒ æŠ“å–å¤±è´¥: {e}")
        
        print(f"\nâœ… æŠ“å–å®Œæˆï¼å…±è·å¾— {len(self.all_articles)} ç¯‡æ–‡ç« ")
        print(f"âœ¨ å…¶ä¸­AIç›¸å…³: {len(self.ai_articles)} ç¯‡")
    
    def analyze_with_zhipu(self, article):
        """ä½¿ç”¨æ™ºè°±AIåˆ†ææ–‡ç« """
        try:
            from zhipuai import ZhipuAI
            
            client = ZhipuAI(api_key=self.zhipu_api_key)
            
            prompt = f"""ä½œä¸ºAIç§‘æŠ€åˆ†æå¸ˆï¼Œè¯·åˆ†æä»¥ä¸‹æ–‡ç« ï¼š

æ ‡é¢˜ï¼š{article['title']}
æ¥æºï¼š{article['source']}
æ‘˜è¦ï¼š{article.get('summary', 'æš‚æ— è¯¦ç»†æ‘˜è¦')}

è¯·æä¾›ä»¥ä¸‹åˆ†æï¼š
1. æ ¸å¿ƒæŠ€æœ¯ç‚¹ï¼ˆè¯†åˆ«æ–‡ä¸­æåˆ°çš„å…³é”®æŠ€æœ¯å¹¶è¿›è¡Œç®€è¦è¯´æ˜ï¼‰
2. åˆ›æ–°ç¨‹åº¦ï¼ˆé«˜/ä¸­/ä½ï¼‰
3. è¡Œä¸šå½±å“ï¼ˆæŠ€æœ¯è¿­ä»£æ–¹å‘ã€æ‹“å±•æ½œåŠ›ã€ç§‘ç ”çªç ´ã€å•†ä¸šåº”ç”¨ã€æŠ€æœ¯æ™®åŠç­‰ï¼‰
4. æ¨èç†ç”±ï¼ˆä¸ºä»€ä¹ˆè¿™ç¯‡æ–‡ç« å€¼å¾—å…³æ³¨ï¼‰
5.æ€§èƒ½è¡¨ç°ï¼ˆæ¨ç†é€Ÿåº¦ã€å‡†ç¡®ç‡ã€å¤šæ¨¡æ€å…¼å®¹æ€§ã€ä¸Šä¸‹æ–‡æ‰¿è½½èƒ½åŠ›ç­‰ï¼‰
6. æŠ€æœ¯æ ‡ç­¾ï¼ˆ3-5ä¸ªå…³é”®è¯ï¼‰

è¯·ç”¨JSONæ ¼å¼å›å¤ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- technique_points: åˆ—è¡¨ï¼Œæ ¸å¿ƒæŠ€æœ¯ç‚¹
- innovation_level: å­—ç¬¦ä¸²ï¼Œé«˜/ä¸­/ä½
- industry_impact: å­—ç¬¦ä¸²
- recommendation_reason: å­—ç¬¦ä¸²
- efficiency_performance: å­—ç¬¦ä¸²
- tech_tags: åˆ—è¡¨ï¼ŒæŠ€æœ¯æ ‡ç­¾
"""
            
            response = client.chat.completions.create(
                model="glm-4",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIç§‘æŠ€åˆ†æå¸ˆï¼Œæ“…é•¿åˆ†ææŠ€æœ¯æ–‡ç« ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=800
            )
            
            result_text = response.choices[0].message.content
            json_match = re.search(r'\{.*\}', result_text, re.DOTALL)
            
            if json_match:
                analysis_result = json.loads(json_match.group())
            else:
                analysis_result = {
                    "technique_points": ["AIæŠ€æœ¯"],
                    "innovation_level": "ä¸­",
                    "industry_impact": "æ¨åŠ¨AIæŠ€æœ¯å‘å±•",
                    "recommendation_reason": "æ–‡ç« æ¶‰åŠå½“å‰AIçƒ­ç‚¹è¯é¢˜",
                    "tech_tags": ["äººå·¥æ™ºèƒ½"]
                }
            
            return {
                'technique_tags': analysis_result.get('tech_tags', ['AIæŠ€æœ¯']),
                'innovation_level': analysis_result.get('innovation_level', 'ä¸­'),
                'industry_impact': analysis_result.get('industry_impact', 'æŠ€æœ¯è¿›å±•'),
                'recommendation': analysis_result.get('recommendation_reason', 'å€¼å¾—å…³æ³¨'),
                'technique_points': analysis_result.get('technique_points', []),
                'source': 'zhipu_ai'
            }
            
        except Exception as e:
            print(f"âš ï¸ æ™ºè°±AIåˆ†æå¤±è´¥: {e}")
            return self._fallback_analysis(article)
    
    def _fallback_analysis(self, article):
        """å¤‡ç”¨å…³é”®è¯åˆ†æ"""
        text = f"{article['title']} {article.get('summary', '')}".lower()
        
        tech_tags = []
        if any(word in text for word in ['llm', 'gpt', 'å¤§è¯­è¨€æ¨¡å‹']):
            tech_tags.append('å¤§è¯­è¨€æ¨¡å‹')
        if any(word in text for word in ['transformer', 'æ³¨æ„åŠ›æœºåˆ¶']):
            tech_tags.append('Transformer')
        if any(word in text for word in ['multimodal', 'å¤šæ¨¡æ€']):
            tech_tags.append('å¤šæ¨¡æ€AI')
        if any(word in text for word in ['computer vision', 'è®¡ç®—æœºè§†è§‰']):
            tech_tags.append('è®¡ç®—æœºè§†è§‰')
        if not tech_tags:
            tech_tags = ['AIæŠ€æœ¯']
        
        return {
            'technique_tags': tech_tags,
            'innovation_level': 'ä¸­',
            'industry_impact': 'æ¨åŠ¨AIæŠ€æœ¯å‘å±•',
            'recommendation': 'AIé¢†åŸŸç›¸å…³è¿›å±•',
            'technique_points': tech_tags,
            'source': 'keyword_analysis'
        }
    
    def generate_deep_analyses(self, limit=5):
        """ç”Ÿæˆæ·±åº¦åˆ†æ"""
        if not self.ai_articles:
            return []
        
        # é€‰æ‹©æœ€é‡è¦çš„æ–‡ç« è¿›è¡Œåˆ†æ
        important_articles = sorted(
            self.ai_articles,
            key=lambda x: x.get('importance', 5),
            reverse=True
        )[:limit]
        
        print(f"\nğŸ” å¼€å§‹æ·±åº¦åˆ†æ {len(important_articles)} ç¯‡æ–‡ç« ...")
        
        analyses = []
        for i, article in enumerate(important_articles, 1):
            print(f"  {i}. åˆ†æ: {article['title'][:60]}...")
            analysis = self.analyze_with_zhipu(article)
            
            analysis_text = f"""## ğŸ“Š {article['title']}

**æ¥æº**: {article['source']} | **æ—¶é—´**: {article.get('time', 'N/A')}
**AIåˆ†ææ¨¡å‹**: ğŸ¤– æ™ºè°±GLM-4

**ğŸ”— åŸæ–‡é“¾æ¥**: {article['link']}

**ğŸ“ å†…å®¹æ‘˜è¦**:
{article.get('summary', 'æš‚æ— è¯¦ç»†æ‘˜è¦')}

**ğŸ·ï¸ æŠ€æœ¯æ ‡ç­¾**: {', '.join(analysis['technique_tags'])}

**âœ¨ åˆ›æ–°ç¨‹åº¦**: {analysis['innovation_level'].upper()}

**ğŸ“ˆ è¡Œä¸šå½±å“**: {analysis['industry_impact']}

**ğŸ’¡ æ¨èç†ç”±**: {analysis['recommendation']}

**ğŸ”¬ æ ¸å¿ƒæŠ€æœ¯ç‚¹**:
{chr(10).join(f'- {point}' for point in analysis['technique_points'][:3])}

---
"""
            analyses.append({
                'article': article,
                'analysis': analysis,
                'text': analysis_text
            })
            time.sleep(1)  # APIè°ƒç”¨é—´éš”
        
        self.deep_analyses = analyses
        return analyses
    
    def select_featured_article(self):
        """é€‰æ‹©æ·±åº¦ç²¾é€‰æ–‡ç« """
        if not self.all_articles:
            return None
        
        # æ ¹æ®é‡è¦æ€§ã€æ¥æºæƒå¨æ€§ã€å†…å®¹é•¿åº¦é€‰æ‹©
        scored_articles = []
        for article in self.all_articles:
            score = article.get('importance', 5)
            
            # æ¥æºæƒé‡
            source_weights = {
                'Arxiv AI Papers': 3,
                'MIT Tech Review AI': 3,
                'æœºå™¨ä¹‹å¿ƒ': 2,
                'é‡å­ä½': 2,
                'TechCrunch AI': 2
            }
            score += source_weights.get(article['source'], 0)
            
            # å†…å®¹é•¿åº¦åŠ åˆ†
            if len(article.get('summary', '')) > 150:
                score += 1
            
            scored_articles.append((score, article))
        
        scored_articles.sort(reverse=True, key=lambda x: x[0])
        self.featured_article = scored_articles[0][1]
        
        return self.featured_article
    
    def generate_report(self):
        """ç”Ÿæˆå®Œæ•´æŠ¥å‘Š"""
        current_time = datetime.now().strftime('%Y-%m-%d %H:%M')
        
        report = f"""# ğŸ¤– AIç§‘æŠ€æ—¥æŠ¥ ({current_time})

## ğŸ“Š æ•°æ®æ¦‚è§ˆ
- æ€»å…±æŠ“å–: **{len(self.all_articles)}** ç¯‡æ–‡ç« 
- AIç›¸å…³: **{len(self.ai_articles)}** ç¯‡
- æ·±åº¦åˆ†æ: **{len(self.deep_analyses)}** ç¯‡
- æ–°é—»æ¥æº: **{len(self.news_sources)}** ä¸ª

"""
        
        # 1. AIå¿«è®¯æ‘˜è¦
        if self.ai_articles:
            report += "\n## ğŸš€ AIå¿«è®¯æ‘˜è¦\n"
            
            # æŒ‰ç±»åˆ«åˆ†ç»„
            articles_by_category = {}
            for article in self.ai_articles[:20]:  # æœ€å¤š20æ¡å¿«è®¯
                cat = article.get('category', 'other')
                if cat not in articles_by_category:
                    articles_by_category[cat] = []
                articles_by_category[cat].append(article)
            
            category_names = {
                'research': 'ğŸ§ª ç ”ç©¶è®ºæ–‡',
                'tech': 'ğŸ“° ç§‘æŠ€æ–°é—»',
                'community': 'ğŸ‘¥ ç¤¾åŒºè®¨è®º',
                'cn_ai': 'ğŸ‡¨ğŸ‡³ ä¸­æ–‡èµ„è®¯'
            }
            
            for cat, articles in articles_by_category.items():
                name = category_names.get(cat, 'ğŸ“Œ å…¶ä»–')
                report += f"\n### {name}\n"
                for i, article in enumerate(articles[:4], 1):
                    report += f"{i}. **{article['title']}**\n"
                    report += f"   ğŸ“ {article['source']} | ğŸ”— [é˜…è¯»åŸæ–‡]({article['link']})\n"
        
        # 2. æ·±åº¦åˆ†æéƒ¨åˆ†
        if self.deep_analyses:
            report += "\n## ğŸ” æ·±åº¦åˆ†æ\n"
            report += "_ä»¥ä¸‹æ–‡ç« å·²è¿›è¡Œè¯¦ç»†æŠ€æœ¯åˆ†æï¼š_\n\n"
            for analysis in self.deep_analyses:
                report += analysis['text']
        
        # 3. æ¯æ—¥ç²¾é€‰
        if self.featured_article:
            report += "\n## ğŸ† ä»Šæ—¥æ·±åº¦ç²¾é€‰\n"
            report += f"### {self.featured_article['title']}\n\n"
            report += f"**æ¥æº**: {self.featured_article['source']}\n"
            report += f"**æ—¶é—´**: {self.featured_article.get('time', 'æœªçŸ¥')}\n"
            report += f"**æ‘˜è¦**: {self.featured_article.get('summary', 'æš‚æ— æ‘˜è¦')}\n\n"
            report += f"**ğŸ”— æ·±åº¦é˜…è¯»**: {self.featured_article['link']}\n"
        
        # 4. è¶‹åŠ¿æ€»ç»“
        report += "\n## ğŸ“ˆ ä»Šæ—¥AIè¶‹åŠ¿æ€»ç»“\n"
        
        # ç»Ÿè®¡æŠ€æœ¯å…³é”®è¯
        all_tags = []
        for analysis in self.deep_analyses:
            all_tags.extend(analysis['analysis']['technique_tags'])
        
        if all_tags:
            tag_counts = Counter(all_tags)
            top_tags = tag_counts.most_common(5)
            
            report += "**çƒ­é—¨æŠ€æœ¯ç„¦ç‚¹**:\n"
            for tag, count in top_tags:
                report += f"- {tag} ({count}æ¬¡æåŠ)\n"
        
        report += f"\n---\n"
        report += f"â° ä¸‹æ¬¡æ›´æ–°: æ˜æ—¥ 08:00 (åŒ—äº¬æ—¶é—´)\n"
        report += f"ğŸ“š æ•°æ®æº: {len(self.news_sources)}ä¸ªä¸“ä¸šAI/ç§‘æŠ€åª’ä½“\n"
        report += f"ğŸ¤– åˆ†ææ–¹å¼: æ™ºè°±GLM-4 AIåˆ†æ\n"
        report += f"ğŸ“… ç”Ÿæˆæ—¶é—´: {current_time}"
        
        return report
    
    def save_report(self, report):
        """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        # ä¿å­˜ä¸ºJSONæ•°æ®
        output_data = {
            'fetch_time': datetime.now().isoformat(),
            'total_articles': len(self.all_articles),
            'ai_articles': len(self.ai_articles),
            'deep_analyses': len(self.deep_analyses),
            'featured_article': self.featured_article,
            'all_articles': self.all_articles[:50]
        }
        
        with open('ai_news_analysis.json', 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜ä¸ºMarkdownæŠ¥å‘Š
        with open('news_report.md', 'w', encoding='utf-8') as f:
            f.write(report)
        
        print("ğŸ’¾ æŠ¥å‘Šå·²ä¿å­˜è‡³: ai_news_analysis.json, news_report.md")
    
    def send_to_wechat(self, report):
        """é€šè¿‡Serveré…±å‘é€åˆ°å¾®ä¿¡"""
        if not self.server_chan_key:
            print("âš ï¸ æœªé…ç½®Serveré…±å¯†é’¥ï¼Œè·³è¿‡æ¨é€")
            return False
        
        # Serveré…±Turboç‰ˆAPI
        url = f"https://sctapi.ftqq.com/{self.server_chan_key}.send"
        
        # å¦‚æœæŠ¥å‘Šè¿‡é•¿ï¼Œè¿›è¡Œæˆªæ–­
        if len(report) > 6000:
            report = report[:6000] + "\n\n...ï¼ˆæŠ¥å‘Šè¿‡é•¿ï¼Œå·²æˆªæ–­ï¼Œå®Œæ•´å†…å®¹è¯·æŸ¥çœ‹ä¿å­˜çš„æ–‡ä»¶ï¼‰"
        
        data = {
            'title': f"AIç§‘æŠ€æ—¥æŠ¥ {datetime.now().strftime('%m-%d')} | {len(self.ai_articles)}ç¯‡AIèµ„è®¯",
            'desp': report
        }
        
        try:
            response = requests.post(url, data=data, timeout=15)
            result = response.json()
            
            if result.get('code') == 0:
                print(f"âœ… å¾®ä¿¡æ¨é€æˆåŠŸï¼æ¶ˆæ¯ID: {result.get('data', {}).get('pushid')}")
                return True
            else:
                print(f"âŒ æ¨é€å¤±è´¥: {result}")
                return False
        except Exception as e:
            print(f"âŒ æ¨é€è¯·æ±‚å¤±è´¥: {e}")
            return False
    
    def run(self):
        """ä¸»æ‰§è¡Œå‡½æ•°"""
        print("=" * 70)
        print("ğŸ¤– AIç§‘æŠ€èµ„è®¯æ™ºèƒ½åˆ†æç³»ç»Ÿå¯åŠ¨")
        print(f"ğŸ“… æ‰§è¡Œæ—¶é—´: {datetime.now()}")
        print("=" * 70)
        
        # 1. æŠ“å–æ–°é—»
        self.fetch_all_news()
        
        if not self.all_articles:
            print("âŒ æœªæŠ“å–åˆ°ä»»ä½•æ–‡ç« ï¼Œç¨‹åºé€€å‡º")
            return None, "æ— å†…å®¹"
        
        # 2. ç”Ÿæˆæ·±åº¦åˆ†æ
        self.generate_deep_analyses(limit=5)
        
        # 3. é€‰æ‹©æ¯æ—¥ç²¾é€‰
        self.select_featured_article()
        
        # 4. ç”ŸæˆæŠ¥å‘Š
        report = self.generate_report()
        
        # 5. ä¿å­˜æŠ¥å‘Š
        self.save_report(report)
        
        # 6. å‘é€æ¨é€
        title = f"AIç§‘æŠ€æ—¥æŠ¥ {datetime.now().strftime('%m-%d')} | {len(self.ai_articles)}ç¯‡AIèµ„è®¯"
        
        return report, title

def main():
    """ä¸»å‡½æ•°"""
    analyzer = AITechNewsAnalyzer()
    report, title = analyzer.run()
    
    if report:
        # å‘é€åˆ°å¾®ä¿¡
        if analyzer.server_chan_key:
            print("\nğŸ“¤ æ­£åœ¨å‘é€åˆ°å¾®ä¿¡...")
            analyzer.send_to_wechat(report)
        else:
            print("\nâš ï¸ æœªé…ç½®SERVER_CHAN_KEYï¼Œè·³è¿‡æ¨é€")
            print("è¯·åœ¨GitHub Secretsä¸­è®¾ç½®è¯¥å¯†é’¥")
        
        # æ‰“å°éƒ¨åˆ†å†…å®¹é¢„è§ˆ
        print("\n" + "=" * 70)
        print("ğŸ“‹ ç”Ÿæˆå†…å®¹é¢„è§ˆ:")
        print("=" * 70)
        print(report[:1500] + "..." if len(report) > 1500 else report)
    else:
        print("âŒ æœªç”ŸæˆæŠ¥å‘Šï¼Œè¯·æ£€æŸ¥é…ç½®")

if __name__ == "__main__":
    main()
