#!/usr/bin/env python3
"""
AIç§‘æŠ€èµ„è®¯æ™ºèƒ½åˆ†æç³»ç»Ÿ
æŠ“å–è¿‡å»24å°æ—¶AI/ç§‘æŠ€èµ„è®¯ï¼Œè¿›è¡Œæ™ºèƒ½åˆ†æï¼Œç”Ÿæˆæ·±åº¦æŠ¥å‘Šå¹¶æ¨é€
"""

import os
import re
import json
import requests
import random
from datetime import datetime, timedelta
import time
from bs4 import BeautifulSoup
from urllib.parse import quote
import hashlib

class AITechNewsAnalyzer:
    def __init__(self):
        self.server_chan_key = os.getenv('SERVER_CHAN_KEY')
        self.zhipu_api_key = os.getenv('ZHIPU_API_KEY')  # æ–°å¢æ™ºè°±APIå¯†é’¥
        self.twenty_four_hours_ago = datetime.now() - timedelta(hours=24)
        # ... å…¶ä½™ä¿æŒä¸å˜

        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        # æ‰©å±•çš„æ–°é—»æº - ä¸“æ³¨AIå’ŒæŠ€æœ¯é¢†åŸŸ
        self.news_sources = [
            # AIä¸“ä¸šæ–°é—»
            {
                'name': 'Arxiv AIæœ€æ–°è®ºæ–‡',
                'url': 'http://arxiv.org/list/cs.AI/recent',
                'type': 'arxiv',
                'category': 'ai_research'
            },
            {
                'name': 'MIT Technology Review AI',
                'url': 'https://www.technologyreview.com/topic/artificial-intelligence/feed/',
                'type': 'rss',
                'category': 'ai_news'
            },
            {
                'name': 'VentureBeat AI',
                'url': 'https://venturebeat.com/category/ai/feed/',
                'type': 'rss',
                'category': 'ai_business'
            },
            # ç»¼åˆç§‘æŠ€æ–°é—»
            {
                'name': 'TechCrunch AI',
                'url': 'https://techcrunch.com/category/artificial-intelligence/feed/',
                'type': 'rss',
                'category': 'tech_news'
            },
            {
                'name': 'The Verge AI',
                'url': 'https://www.theverge.com/ai-artificial-intelligence/rss',
                'type': 'rss',
                'category': 'tech_news'
            },
            {
                'name': 'Hacker News AI',
                'url': 'https://hn.algolia.com/api/v1/search_by_date?tags=story&numericFilters=created_at_i>{}&query=AI',
                'type': 'api',
                'category': 'community'
            },
            # ä¸­æ–‡AIæ–°é—»
            {
                'name': 'æœºå™¨ä¹‹å¿ƒ',
                'url': 'https://www.jiqizhixin.com/feed',
                'type': 'rss',
                'category': 'ai_news_cn'
            },
            {
                'name': 'é‡å­ä½AI',
                'url': 'https://www.qbitai.com/feed',
                'type': 'rss',
                'category': 'ai_news_cn'
            }
        ]
        
        self.all_articles = []
        self.ai_articles = []
        self.deep_analysis = []
        self.featured_article = None

    
    def fetch_arxiv_papers(self, source):
        """æŠ“å–Arxiv AIæœ€æ–°è®ºæ–‡"""
        try:
            response = requests.get(source['url'], headers=self.headers, timeout=15)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # è§£æArxivé¡µé¢ç»“æ„
                dt_list = soup.find_all('dt')
                dd_list = soup.find_all('dd')
                
                for i, (dt, dd) in enumerate(zip(dt_list[:10], dd_list[:10])):
                    # æå–è®ºæ–‡IDå’Œæ ‡é¢˜
                    paper_id = dt.find('a', title='Abstract').text.strip()
                    title_tag = dd.find('div', class_='list-title')
                    if title_tag:
                        title = title_tag.text.replace('Title:', '').strip()
                        
                        # æå–ä½œè€…å’Œæ‘˜è¦
                        authors_tag = dd.find('div', class_='list-authors')
                        authors = authors_tag.text.replace('Authors:', '').strip() if authors_tag else ''
                        
                        abstract_tag = dd.find('p')
                        abstract = abstract_tag.text.strip() if abstract_tag else ''
                        
                        # ç”ŸæˆArxivé“¾æ¥
                        paper_url = f'https://arxiv.org/abs/{paper_id}'
                        
                        article = {
                            'title': f"[è®ºæ–‡] {title[:80]}",
                            'link': paper_url,
                            'source': source['name'],
                            'summary': abstract[:150] + '...' if len(abstract) > 150 else abstract,
                            'authors': authors,
                            'category': 'ai_research',
                            'importance': 8,  # é‡è¦æ€§è¯„åˆ†(1-10)
                            'time': datetime.now().strftime('%Y-%m-%d')
                        }
                        
                        self.all_articles.append(article)
                        self.ai_articles.append(article)
                        
        except Exception as e:
            print(f"ArxivæŠ“å–å¤±è´¥: {e}")
    
    def fetch_rss_feed(self, source):
        """æŠ“å–RSSæ–°é—»æº"""
        try:
            import feedparser
            feed = feedparser.parse(source['url'])
            
            for entry in feed.entries[:8]:  # æ¯ä¸ªæºå–å‰8æ¡
                # æ£€æŸ¥æ˜¯å¦åŒ…å«AIå…³é”®è¯
                title = entry.get('title', '')
                summary = entry.get('summary', '')
                content = f"{title} {summary}".lower()
                
                ai_keywords = ['ai', 'artificial intelligence', 'machine learning', 
                              'æ·±åº¦å­¦ä¹ ', 'ç¥ç»ç½‘ç»œ', 'llm', 'gpt', 'äººå·¥æ™ºèƒ½']
                
                is_ai_related = any(keyword in content for keyword in ai_keywords)
                
                article = {
                    'title': title[:100],
                    'link': entry.get('link', ''),
                    'source': source['name'],
                    'summary': summary[:200] + '...' if len(summary) > 200 else summary,
                    'category': source['category'],
                    'importance': 7 if is_ai_related else 5,
                    'time': entry.get('published', datetime.now().strftime('%Y-%m-%d'))
                }
                
                self.all_articles.append(article)
                if is_ai_related:
                    self.ai_articles.append(article)
                    
        except Exception as e:
            print(f"RSSæŠ“å–å¤±è´¥ {source['name']}: {e}")
    
    def fetch_hackernews_ai(self, source):
        """æŠ“å–Hacker News AIç›¸å…³å†…å®¹"""
        try:
            # è®¡ç®—æ—¶é—´æˆ³
            timestamp = int((datetime.now() - timedelta(hours=24)).timestamp())
            url = source['url'].format(timestamp)
            
            response = requests.get(url, timeout=10)
            if response.status_code == 200:
                hits = response.json().get('hits', [])
                
                for hit in hits[:15]:
                    title = hit.get('title', '').lower()
                    
                    # ç­›é€‰AIç›¸å…³å†…å®¹
                    if any(keyword in title for keyword in ['ai', 'llm', 'gpt', 'openai', 'anthropic']):
                        article = {
                            'title': hit.get('title', ''),
                            'link': hit.get('url', f"https://news.ycombinator.com/item?id={hit.get('objectID')}"),
                            'source': source['name'],
                            'points': hit.get('points', 0),
                            'comments': hit.get('num_comments', 0),
                            'category': 'community',
                            'importance': min(9, 6 + (hit.get('points', 0) // 10)),  # æ ¹æ®ç‚¹èµæ•°è¯„åˆ†
                            'time': datetime.fromtimestamp(hit.get('created_at_i', 0)).strftime('%Y-%m-%d %H:%M')
                        }
                        
                        self.all_articles.append(article)
                        self.ai_articles.append(article)
                        
        except Exception as e:
            print(f"Hacker NewsæŠ“å–å¤±è´¥: {e}")
    
def analyze_with_ai(self, article):
    """ä½¿ç”¨æ™ºè°±AIåˆ†ææ–‡ç« å†…å®¹"""
    try:
        # ä¼˜å…ˆä½¿ç”¨æ™ºè°±AI
        if self.zhipu_api_key:
            return self._analyze_with_zhipu(article)
        # å¤‡ç”¨ï¼šå…³é”®è¯åˆ†æ
        else:
            return self._analyze_with_keywords(article)
    except Exception as e:
        print(f"AIåˆ†æå¤±è´¥: {e}")
        return self._analyze_with_keywords(article)
    
    def _analyze_with_keywords(self, article):
        """åŸºäºå…³é”®è¯çš„ç®€å•åˆ†æ"""
        title = article['title'].lower()
        summary = article.get('summary', '').lower()
        text = f"{title} {summary}"
        
        analysis = {
            'technique_tags': [],
            'trend_insight': '',
            'business_impact': '',
            'difficulty': 'medium'
        }
        
        # æŠ€æœ¯å…³é”®è¯æ£€æµ‹
        tech_keywords = {
            'llm': 'å¤§è¯­è¨€æ¨¡å‹',
            'gpt': 'GPTç³»åˆ—',
            'diffusion': 'æ‰©æ•£æ¨¡å‹',
            'transformer': 'Transformeræ¶æ„',
            'multimodal': 'å¤šæ¨¡æ€AI',
            'reinforcement': 'å¼ºåŒ–å­¦ä¹ ',
            'computer vision': 'è®¡ç®—æœºè§†è§‰',
            'nlp': 'è‡ªç„¶è¯­è¨€å¤„ç†'
        }
        
        for eng, chi in tech_keywords.items():
            if eng in text:
                analysis['technique_tags'].append(chi)
        
        # è¶‹åŠ¿æ´å¯Ÿ
        if not analysis['technique_tags']:
            analysis['technique_tags'] = ['AIæŠ€æœ¯']
        
        if any(word in text for word in ['breakthrough', 'new method', 'åˆ›æ–°']):
            analysis['trend_insight'] = 'æŠ€æœ¯çªç ´æ€§è¿›å±•'
            analysis['importance'] = 9
        elif any(word in text for word in ['application', 'deploy', 'åº”ç”¨']):
            analysis['trend_insight'] = 'å®é™…åº”ç”¨éƒ¨ç½²'
            analysis['importance'] = 8
        else:
            analysis['trend_insight'] = 'æŠ€æœ¯ç ”ç©¶è¿›å±•'
            analysis['importance'] = 7
        
        return analysis
    def _analyze_with_zhipu(self, article):
    """ä½¿ç”¨æ™ºè°±AI GLMæ¨¡å‹è¿›è¡Œåˆ†æ"""
    try:
        from zhipuai import ZhipuAI
        
        # åˆå§‹åŒ–æ™ºè°±å®¢æˆ·ç«¯
        client = ZhipuAI(api_key=self.zhipu_api_key)
        
        # æ„å»ºåˆ†ææç¤ºè¯
        prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªAIç§‘æŠ€åˆ†æå¸ˆã€‚è¯·åˆ†æä»¥ä¸‹ç§‘æŠ€æ–‡ç« ï¼Œæä¾›ç»“æ„åŒ–åˆ†æã€‚
        
        æ–‡ç« æ ‡é¢˜ï¼š{article['title']}
        æ–‡ç« æ¥æºï¼š{article['source']}
        æ–‡ç« æ‘˜è¦ï¼š{article.get('summary', 'æš‚æ— è¯¦ç»†æ‘˜è¦')}
        
        è¯·æä¾›ä»¥ä¸‹åˆ†æï¼š
        1. æ ¸å¿ƒæŠ€æœ¯ç‚¹ï¼ˆè¯†åˆ«æ–‡ä¸­æåˆ°çš„å…³é”®æŠ€æœ¯ï¼Œå¦‚Transformerã€LLMã€å¤šæ¨¡æ€ç­‰ï¼‰
        2. åˆ›æ–°ç¨‹åº¦ï¼ˆé«˜/ä¸­/ä½ï¼‰
        3. è¡Œä¸šå½±å“ï¼ˆç§‘ç ”çªç ´ã€å•†ä¸šåº”ç”¨ã€æŠ€æœ¯æ™®åŠç­‰ï¼‰
        4. æ¨èç†ç”±ï¼ˆä¸ºä»€ä¹ˆè¿™ç¯‡æ–‡ç« å€¼å¾—å…³æ³¨ï¼‰
        5. æŠ€æœ¯æ ‡ç­¾ï¼ˆ3-5ä¸ªå…³é”®è¯ï¼‰
        
        è¯·ç”¨JSONæ ¼å¼å›å¤ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
        - technique_points: åˆ—è¡¨ï¼Œæ ¸å¿ƒæŠ€æœ¯ç‚¹
        - innovation_level: å­—ç¬¦ä¸²ï¼Œé«˜/ä¸­/ä½
        - industry_impact: å­—ç¬¦ä¸²
        - recommendation_reason: å­—ç¬¦ä¸²
        - tech_tags: åˆ—è¡¨ï¼ŒæŠ€æœ¯æ ‡ç­¾
        - summary: å­—ç¬¦ä¸²ï¼Œä¸€å¥è¯æ€»ç»“
        
        æ³¨æ„ï¼šä¿æŒåˆ†æå®¢è§‚ä¸“ä¸šï¼Œå¦‚æœä¿¡æ¯ä¸è¶³è¯·åˆç†æ¨æ–­ã€‚
        """
        
        # è°ƒç”¨æ™ºè°±GLMæ¨¡å‹
        response = client.chat.completions.create(
            model="glm-4",  # ä½¿ç”¨GLM-4æ¨¡å‹ï¼Œä¹Ÿå¯ç”¨"glm-3-turbo"
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIç§‘æŠ€åˆ†æå¸ˆï¼Œæ“…é•¿åˆ†ææŠ€æœ¯æ–‡ç« ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=800
        )
        
        # è§£æè¿”å›å†…å®¹
        result_text = response.choices[0].message.content
        
        # æå–JSONéƒ¨åˆ†ï¼ˆæ™ºè°±å¯èƒ½ä¼šåœ¨JSONå¤–æ·»åŠ è¯´æ˜æ–‡å­—ï¼‰
        import re
        json_match = re.search(r'\{.*\}', result_text, re.DOTALL)
        if json_match:
            import json
            analysis_result = json.loads(json_match.group())
        else:
            # å¦‚æœè¿”å›çš„ä¸æ˜¯çº¯JSONï¼Œä½¿ç”¨é»˜è®¤ç»“æ„
            analysis_result = {
                "technique_points": ["AIæŠ€æœ¯"],
                "innovation_level": "ä¸­",
                "industry_impact": "æ¨åŠ¨AIæŠ€æœ¯å‘å±•",
                "recommendation_reason": "æ–‡ç« æ¶‰åŠå½“å‰AIçƒ­ç‚¹è¯é¢˜",
                "tech_tags": ["äººå·¥æ™ºèƒ½"],
                "summary": f"{article['title']} - AIé¢†åŸŸç›¸å…³è¿›å±•"
            }
        
        # è½¬æ¢ä¸ºè„šæœ¬éœ€è¦çš„æ ¼å¼
        return {
            'technique_tags': analysis_result.get('tech_tags', ['AIæŠ€æœ¯']),
            'trend_insight': analysis_result.get('industry_impact', 'æŠ€æœ¯è¿›å±•'),
            'business_impact': analysis_result.get('recommendation_reason', 'è¡Œä¸šå…³æ³¨'),
            'difficulty': self._map_innovation_to_difficulty(analysis_result.get('innovation_level', 'ä¸­')),
            'ai_summary': analysis_result.get('summary', ''),
            'innovation_level': analysis_result.get('innovation_level', 'ä¸­'),
            'source': 'zhipu_ai'
        }
        
    except Exception as e:
        print(f"æ™ºè°±AIåˆ†æå¤±è´¥: {e}")
        # é™çº§åˆ°å…³é”®è¯åˆ†æ
        return self._analyze_with_keywords(article)

def _map_innovation_to_difficulty(self, level):
    """å°†åˆ›æ–°ç¨‹åº¦æ˜ å°„ä¸ºæŠ€æœ¯éš¾åº¦"""
    mapping = {
        'é«˜': 'high',
        'ä¸­': 'medium', 
        'ä½': 'low'
    }
    return mapping.get(level, 'medium')
    def _analyze_with_openai(self, article):
        """ä½¿ç”¨OpenAIå…¼å®¹APIè¿›è¡Œåˆ†æï¼ˆéœ€è¦APIå¯†é’¥ï¼‰"""
        try:
            import openai
            
            # ä½¿ç”¨OpenRouterä½œä¸ºç¤ºä¾‹ï¼ˆæ”¯æŒå¤šä¸ªæ¨¡å‹ï¼‰
            client = openai.OpenAI(
                base_url="https://openrouter.ai/api/v1",
                api_key=self.ai_api_key
            )
            
            prompt = f"""
            è¯·åˆ†æä»¥ä¸‹AI/ç§‘æŠ€æ–‡ç« ï¼Œæä¾›ï¼š
            1. æ ¸å¿ƒæŠ€æœ¯ç‚¹ï¼ˆ3-5ä¸ªå…³é”®è¯ï¼‰
            2. è¡Œä¸šå½±å“åˆ†æ
            3. æŠ€æœ¯éš¾åº¦è¯„çº§ï¼ˆlow/medium/highï¼‰
            4. ä¸€å¥è¯æ€»ç»“
            
            æ–‡ç« æ ‡é¢˜ï¼š{article['title']}
            æ–‡ç« æ‘˜è¦ï¼š{article.get('summary', 'æ— æ‘˜è¦')}
            æ¥æºï¼š{article['source']}
            
            è¯·ç”¨JSONæ ¼å¼å›å¤ï¼ŒåŒ…å«ï¼štechnique_tags, industry_impact, difficulty_level, summaryã€‚
            """
            
            response = client.chat.completions.create(
                model="google/gemma-7b-it:free",  # å…è´¹æ¨¡å‹
                messages=[{"role": "user", "content": prompt}],
                max_tokens=500,
                temperature=0.3
            )
            
            # è§£æè¿”å›çš„JSON
            import json
            result = json.loads(response.choices[0].message.content)
            return result
            
        except Exception as e:
            print(f"OpenAIåˆ†æå¤±è´¥ï¼Œé€€å›å…³é”®è¯åˆ†æ: {e}")
            return self._analyze_with_keywords(article)
    
    def select_featured_article(self):
        """é€‰æ‹©ä¸€ç¯‡æ·±åº¦ç²¾é€‰æ–‡ç« """
        if not self.all_articles:
            return None
        
        # æ ¹æ®é‡è¦æ€§ã€æ¥æºæƒå¨æ€§ã€å†…å®¹é•¿åº¦ç­‰è¯„åˆ†
        scored_articles = []
        for article in self.all_articles:
            score = article.get('importance', 5)
            
            # æ¥æºæƒå¨æ€§åŠ åˆ†
            source_weights = {
                'Arxiv AIæœ€æ–°è®ºæ–‡': 2,
                'MIT Technology Review AI': 3,
                'æœºå™¨ä¹‹å¿ƒ': 2,
                'é‡å­ä½AI': 2,
                'VentureBeat AI': 2
            }
            score += source_weights.get(article['source'], 0)
            
            # å†…å®¹é•¿åº¦åŠ åˆ†
            if len(article.get('summary', '')) > 200:
                score += 1
            
            scored_articles.append((score, article))
        
        # é€‰æ‹©åˆ†æ•°æœ€é«˜çš„
        scored_articles.sort(reverse=True, key=lambda x: x[0])
        self.featured_article = scored_articles[0][1]
        
        return self.featured_article
    
    def generate_detailed_analysis(self, limit=5):
        """ç”Ÿæˆè¯¦ç»†åˆ†ææ–‡ç¨¿"""
        if not self.ai_articles:
            return []
        
        # é€‰æ‹©æœ€é‡è¦çš„å‡ ç¯‡è¿›è¡Œåˆ†æ
        important_articles = sorted(
            self.ai_articles, 
            key=lambda x: x.get('importance', 5), 
            reverse=True
        )[:limit]
        
        analyses = []
        for article in important_articles:
            analysis = self.analyze_with_ai(article)
            
            analysis_text = f"""
## ğŸ“Š {article['title']}

**æ¥æº**: {article['source']} | **æ—¶é—´**: {article.get('time', 'N/A')}

**ğŸ”— åŸæ–‡é“¾æ¥**: {article['link']}

**ğŸ“ å†…å®¹æ‘˜è¦**:
{article.get('summary', 'æš‚æ— è¯¦ç»†æ‘˜è¦')}

**ğŸ·ï¸ æŠ€æœ¯æ ‡ç­¾**: {', '.join(analysis.get('technique_tags', ['AIæŠ€æœ¯']))}

**ğŸ“ˆ è¶‹åŠ¿æ´å¯Ÿ**: {analysis.get('trend_insight', 'AIé¢†åŸŸè¿›å±•')}

**ğŸ’¼ è¡Œä¸šå½±å“**: {analysis.get('business_impact', 'æ¨åŠ¨AIæŠ€æœ¯å‘å±•ä¸åº”ç”¨')}

**âš™ï¸ æŠ€æœ¯éš¾åº¦**: {analysis.get('difficulty', 'medium').upper()}

---
"""
            analyses.append({
                'article': article,
                'analysis': analysis,
                'text': analysis_text
            })
        
        self.deep_analysis = analyses
        return analyses
    
    def format_push_message(self):
        """æ ¼å¼åŒ–æ¨é€æ¶ˆæ¯"""
        current_time = datetime.now().strftime('%Y-%m-%d %H:%M')
        
        message = f"""# ğŸ¤– AIç§‘æŠ€æ—¥æŠ¥ ({current_time})

# åœ¨æ·±åº¦åˆ†æéƒ¨åˆ†ï¼Œæ›´æ–°åˆ†ææ–‡æœ¬ç”Ÿæˆ
analysis_text = f"""
## ğŸ“Š {article['title']}

**æ¥æº**: {article['source']} | **æ—¶é—´**: {article.get('time', 'N/A')}
**AIåˆ†ææ¨¡å‹**: ğŸ¤– æ™ºè°±GLM-4

**ğŸ”— åŸæ–‡é“¾æ¥**: {article['link']}

**ğŸ“ å†…å®¹æ‘˜è¦**:
{article.get('summary', 'æš‚æ— è¯¦ç»†æ‘˜è¦')}

**ğŸ·ï¸ æŠ€æœ¯æ ‡ç­¾**: {', '.join(analysis.get('technique_tags', ['AIæŠ€æœ¯']))}

**âœ¨ åˆ›æ–°ç¨‹åº¦**: {analysis.get('innovation_level', 'ä¸­').upper()}

**ğŸ“ˆ è¶‹åŠ¿æ´å¯Ÿ**: {analysis.get('trend_insight', 'AIé¢†åŸŸè¿›å±•')}

**ğŸ’¼ è¡Œä¸šå½±å“**: {analysis.get('business_impact', 'æ¨åŠ¨AIæŠ€æœ¯å‘å±•ä¸åº”ç”¨')}

**âš™ï¸ æŠ€æœ¯éš¾åº¦**: {analysis.get('difficulty', 'medium').upper()}

**ğŸ¤– AIåˆ†ææ‘˜è¦**: {analysis.get('ai_summary', '')}

---
"""

        # 1. AIå¿«è®¯æ‘˜è¦
        if self.ai_articles:
            message += "\n## ğŸš€ AIå¿«è®¯æ‘˜è¦\n"
            ai_by_category = {}
            for article in self.ai_articles[:15]:  # æœ€å¤š15æ¡å¿«è®¯
                cat = article.get('category', 'other')
                if cat not in ai_by_category:
                    ai_by_category[cat] = []
                ai_by_category[cat].append(article)
            
            for category, articles in ai_by_category.items():
                category_name = {
                    'ai_research': 'ğŸ§ª ç ”ç©¶è®ºæ–‡',
                    'ai_news': 'ğŸ“° AIæ–°é—»',
                    'ai_business': 'ğŸ’¼ å•†ä¸šåº”ç”¨',
                    'tech_news': 'ğŸ”§ æŠ€æœ¯åŠ¨æ€',
                    'ai_news_cn': 'ğŸ‡¨ğŸ‡³ ä¸­æ–‡èµ„è®¯'
                }.get(category, 'ğŸ“Œ å…¶ä»–')
                
                message += f"\n### {category_name}\n"
                for i, article in enumerate(articles[:4], 1):
                    message += f"{i}. **{article['title']}**\n"
                    message += f"   ğŸ“ {article['source']} | ğŸ”— [é˜…è¯»åŸæ–‡]({article['link']})\n"
        
        # 2. æ·±åº¦åˆ†æéƒ¨åˆ†
        if self.deep_analysis:
            message += "\n## ğŸ” æ·±åº¦åˆ†æ\n"
            message += "_ä»¥ä¸‹æ–‡ç« å·²è¿›è¡Œè¯¦ç»†æŠ€æœ¯åˆ†æï¼š_\n\n"
            
            for analysis in self.deep_analysis:
                article = analysis['article']
                message += f"### {article['title']}\n"
                message += analysis['text']
        
        # 3. æ¯æ—¥ç²¾é€‰
        if self.featured_article:
            message += "\n## ğŸ† ä»Šæ—¥æ·±åº¦ç²¾é€‰\n"
            message += f"### {self.featured_article['title']}\n\n"
            message += f"**æ¨èç†ç”±**: æœ¬æ–‡æ¥è‡ª{self.featured_article['source']}ï¼Œ"
            message += f"åœ¨ä»Šæ—¥èµ„è®¯ä¸­å…·æœ‰è¾ƒé«˜çš„æŠ€æœ¯æ·±åº¦å’Œè¡Œä¸šå½±å“åŠ›ã€‚\n\n"
            message += f"**æ ¸å¿ƒè¦ç‚¹**:\n"
            
            # ä»æ‘˜è¦ä¸­æå–è¦ç‚¹
            summary = self.featured_article.get('summary', '')
            sentences = summary.split('. ')
            for i, sentence in enumerate(sentences[:3], 1):
                if sentence.strip():
                    message += f"{i}. {sentence.strip()}.\n"
            
            message += f"\n**ğŸ”— æ·±åº¦é˜…è¯»**: {self.featured_article['link']}\n"
        
        # 4. è¶‹åŠ¿æ€»ç»“
        message += "\n## ğŸ“ˆ ä»Šæ—¥AIè¶‹åŠ¿æ€»ç»“\n"
        
        # ç»Ÿè®¡æŠ€æœ¯å…³é”®è¯
        all_tags = []
        for analysis in self.deep_analysis:
            all_tags.extend(analysis['analysis'].get('technique_tags', []))
        
        if all_tags:
            from collections import Counter
            tag_counts = Counter(all_tags)
            top_tags = tag_counts.most_common(5)
            
            message += "**çƒ­é—¨æŠ€æœ¯ç„¦ç‚¹**:\n"
            for tag, count in top_tags:
                message += f"â€¢ {tag} ({count}æ¬¡æåŠ)\n"
        
        message += f"\n---\n"
        message += f"â° ä¸‹æ¬¡æ›´æ–°: æ˜æ—¥ 08:00 (åŒ—äº¬æ—¶é—´)\n"
        message += f"ğŸ“š æ•°æ®æº: {len(self.news_sources)}ä¸ªä¸“ä¸šAI/ç§‘æŠ€åª’ä½“\n"
        message += f"ğŸ¤– åˆ†ææ–¹å¼: å…³é”®è¯åˆ†æ"
        if self.ai_api_key:
            message += "+AIæ¨¡å‹åˆ†æ"
        
        title = f"AIç§‘æŠ€æ—¥æŠ¥ {current_time.split()[0]} | {len(self.ai_articles)}ç¯‡AIèµ„è®¯"
        
        return message, title
    
    def run(self):
        """ä¸»æ‰§è¡Œå‡½æ•°"""
        print("=" * 70)
        print("ğŸ¤– AIç§‘æŠ€èµ„è®¯æ™ºèƒ½åˆ†æç³»ç»Ÿå¯åŠ¨")
        print(f"ğŸ“… æ‰§è¡Œæ—¶é—´: {datetime.now()}")
        print("=" * 70)
        
        # é¡ºåºæŠ“å–å„æ–°é—»æº
        print("\nğŸ“¡ å¼€å§‹æŠ“å–æ–°é—»æº...")
        for source in self.news_sources:
            print(f"  â†’ æ­£åœ¨æŠ“å–: {source['name']}")
            
            if source['type'] == 'arxiv':
                self.fetch_arxiv_papers(source)
            elif source['type'] == 'rss':
                self.fetch_rss_feed(source)
            elif source['type'] == 'api':
                self.fetch_hackernews_ai(source)
            
            time.sleep(1.5)  # ç¤¼è²Œå»¶è¿Ÿ
        
        print(f"\nâœ… æŠ“å–å®Œæˆï¼å…±è·å¾— {len(self.all_articles)} ç¯‡æ–‡ç« ")
        print(f"âœ¨ å…¶ä¸­AIç›¸å…³: {len(self.ai_articles)} ç¯‡")
        
        # ç”Ÿæˆæ·±åº¦åˆ†æ
        print("\nğŸ” å¼€å§‹æ·±åº¦åˆ†æé‡è¦æ–‡ç« ...")
        self.generate_detailed_analysis(limit=5)
        
        # é€‰æ‹©æ¯æ—¥ç²¾é€‰
        print("\nğŸ† é€‰æ‹©ä»Šæ—¥æ·±åº¦ç²¾é€‰...")
        self.select_featured_article()
        
        # ç”Ÿæˆæ¨é€æ¶ˆæ¯
        print("\nğŸ“ ç”Ÿæˆæ¨é€å†…å®¹...")
        message, title = self.format_push_message()
        
        # ä¿å­˜åˆ†æç»“æœ
        output = {
            'fetch_time': datetime.now().isoformat(),
            'total_articles': len(self.all_articles),
            'ai_articles': len(self.ai_articles),
            'deep_analysis': len(self.deep_analysis),
            'featured_article': self.featured_article,
            'articles': self.all_articles[:20]
        }
        
        with open('ai_news_analysis.json', 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ åˆ†æç»“æœå·²ä¿å­˜è‡³: ai_news_analysis.json")
        print(f"ğŸ“¨ æ¶ˆæ¯æ ‡é¢˜: {title}")
        print(f"ğŸ“ æ¶ˆæ¯é•¿åº¦: {len(message)} å­—ç¬¦")
        
        return message, title

def send_to_serverchan(title, message, api_key):
    """å‘é€åˆ°Serveré…±"""
    if not api_key:
        print("âŒ æœªé…ç½®Serveré…±å¯†é’¥ï¼Œè·³è¿‡æ¨é€")
        return False
    
    # å¦‚æœæ¶ˆæ¯è¿‡é•¿ï¼Œè¿›è¡Œåˆ†å‰²ï¼ˆServeré…±æœ‰é™åˆ¶ï¼‰
    if len(message) > 6000:
        parts = [message[i:i+4000] for i in range(0, len(message), 4000)]
        message = parts[0] + f"\n\n...ï¼ˆæ¶ˆæ¯è¿‡é•¿ï¼Œå·²æˆªæ–­ï¼Œå®Œæ•´å†…å®¹è¯·æŸ¥çœ‹æ—¥å¿—ï¼‰"
    
    url = f"https://sctapi.ftqq.com/{api_key}.send"
    
    data = {
        'title': title[:100],  # æ ‡é¢˜é™åˆ¶é•¿åº¦
        'desp': message,
        'channel': 9  # ä¼ä¸šå¾®ä¿¡é€šé“ï¼Œæ›´ç¨³å®š
    }
    
    try:
        response = requests.post(url, data=data, timeout=15)
        result = response.json()
        
        if result.get('code') == 0:
            print(f"âœ… å¾®ä¿¡æ¨é€æˆåŠŸï¼æ¨é€ID: {result.get('data', {}).get('pushid')}")
            return True
        else:
            print(f"âŒ æ¨é€å¤±è´¥: {result}")
            return False
    except Exception as e:
        print(f"âŒ æ¨é€è¯·æ±‚å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    analyzer = AITechNewsAnalyzer()
    message, title = analyzer.run()
    
    # å‘é€æ¨é€
    api_key = os.getenv('SERVER_CHAN_KEY')
    if api_key:
        print("\nğŸ“¤ æ­£åœ¨å‘é€åˆ°å¾®ä¿¡...")
        success = send_to_serverchan(title, message, api_key)
        if not success:
            print("\nâš ï¸ æ¨é€å¤±è´¥ï¼Œä½†åˆ†æå·²å®Œæˆã€‚")
    else:
        print("\nâš ï¸ æœªé…ç½®SERVER_CHAN_KEYï¼Œè·³è¿‡æ¨é€")
        print("è¯·åœ¨GitHub Secretsä¸­æ·»åŠ è¯¥å¯†é’¥")
    
    # åœ¨æ§åˆ¶å°æ˜¾ç¤ºéƒ¨åˆ†å†…å®¹
    print("\n" + "=" * 70)
    print("ğŸ“‹ ç”Ÿæˆå†…å®¹é¢„è§ˆ:")
    print("=" * 70)
    print(message[:1500] + "..." if len(message) > 1500 else message)
