#!/usr/bin/env python3
"""
AIç§‘æŠ€èµ„è®¯ä¸äº‹å®èµ„è®¯æ™ºèƒ½åˆ†æç³»ç»Ÿ
æŠ“å–è¿‡å»48å°æ—¶AI/ç§‘æŠ€èµ„è®¯å’Œå¤šæ–¹äº‹å®æ–°é—»ï¼Œæ™ºèƒ½åˆ†æåæ¨é€
"""

import os
import re
import json
import requests
import hashlib
from datetime import datetime, timedelta
import time
from bs4 import BeautifulSoup
import feedparser
from urllib.parse import urljoin
from collections import Counter

class EnhancedNewsAnalyzer:
    def __init__(self):
        self.server_chan_key = os.getenv('SERVER_CHAN_KEY')
        self.zhipu_api_key = os.getenv('ZHIPU_API_KEY')
        self.forty_eight_hours_ago = datetime.now() - timedelta(hours=48)
        
        # AIç§‘æŠ€æ–°é—»æºï¼ˆä¿æŒä¸å˜ï¼‰
        self.ai_news_sources = [
            {'name': 'Arxiv AI Papers', 'url': 'http://arxiv.org/list/cs.AI/recent', 'type': 'arxiv', 'category': 'ai_research'},
            {'name': 'TechCrunch AI', 'url': 'https://techcrunch.com/category/artificial-intelligence/feed/', 'type': 'rss', 'category': 'tech'},
            {'name': 'Hacker News AI', 'url': 'https://hn.algolia.com/api/v1/search_by_date?tags=story&numericFilters=created_at_i>{}&query=AI', 'type': 'hn_api', 'category': 'community'},
            {'name': 'æœºå™¨ä¹‹å¿ƒ', 'url': 'https://www.jiqizhixin.com/feed', 'type': 'rss', 'category': 'cn_ai'},
            {'name': 'é‡å­ä½', 'url': 'https://www.qbitai.com/feed', 'type': 'rss', 'category': 'cn_ai'},
        ]
        
        # æ›´æ–°ï¼šå¤šæ–¹é¢äº‹å®æ–°é—»æºï¼Œä¼˜å…ˆä¸­å›½å›½å†…å¯è®¿é—®æ¥æºï¼Œå‡å°‘é‡å¤å’Œè¿‡æ—¶
        self.fact_news_sources = [
            # å›½å†…æ–°é—»ï¼ˆä¼˜å…ˆå¯è®¿é—®æ¥æºï¼‰
            {'name': 'å¤®è§†ç½‘', 'url': 'http://news.cctv.com/rss/index.xml', 'type': 'rss', 'category': 'china', 'lang': 'zh'},
            {'name': 'æ–°åç½‘', 'url': 'http://www.news.cn/rss/rsstw.xml', 'type': 'rss', 'category': 'china', 'lang': 'zh'},  # æ›´æ–°ä¸ºæ›´ç¨³å®šçš„æ–°åç½‘RSS
            {'name': 'äººæ°‘æ—¥æŠ¥', 'url': 'http://www.people.com.cn/rss/politics.xml', 'type': 'rss', 'category': 'china', 'lang': 'zh'},
            {'name': 'æ¾æ¹ƒæ–°é—»', 'url': 'https://rsshub.app/thepaper/featured', 'type': 'rss', 'category': 'china', 'lang': 'zh'},
            {'name': 'è™æ‰‘ç¤¾åŒº', 'url': 'https://rsshub.app/hupu/bbs/all', 'type': 'rss', 'category': 'community', 'lang': 'zh'},  # æ·»åŠ è™æ‰‘ via RSSHub
            {'name': 'è…¾è®¯æ–°é—»', 'url': 'https://rsshub.app/tencent/news/author/1', 'type': 'rss', 'category': 'china', 'lang': 'zh'},  # æ·»åŠ è…¾è®¯æ–°é—»
            # å›½é™…/äºšå¤ªæ–°é—»ï¼ˆé€‰æ‹©åœ¨ä¸­å›½å¯è®¿é—®æˆ–ä¸­ç«‹æ¥æºï¼‰
            {'name': 'è”åˆæ—©æŠ¥', 'url': 'https://www.zaobao.com/realtime/china/rss', 'type': 'rss', 'category': 'asia', 'lang': 'zh'},  # æ›´æ–°ä¸ºä¸­å›½å®æ—¶
            {'name': 'BBCä¸­æ–‡', 'url': 'https://feeds.bbci.co.uk/zhongwen/simp/rss.xml', 'type': 'rss', 'category': 'world', 'lang': 'zh'},  # BBCä¸­æ–‡ç‰ˆï¼Œå¯è®¿é—®
            {'name': 'Reuters China', 'url': 'https://www.reuters.com/arc/outboundfeeds/rss/world/china/', 'type': 'rss', 'category': 'world', 'lang': 'en'},  # Reutersä¸­å›½ç›¸å…³
            # ç¤¾åŒº/ç»¼åˆ
            {'name': 'Reddit World News', 'url': 'https://www.reddit.com/r/worldnews/.rss', 'type': 'rss', 'category': 'world', 'lang': 'en'},
            {'name': 'Hacker News Top', 'url': 'https://hn.algolia.com/api/v1/search_by_date?tags=story&numericFilters=created_at_i>{}', 'type': 'hn_api', 'category': 'tech', 'lang': 'en'},
        ]
        
        self.all_articles = []
        self.ai_articles = []
        self.fact_articles = []
        self.deep_analyses = []
        self.featured_article = None
        self.featured_fact = None
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
    
    # ==================== åŸæœ‰AIæ–°é—»æŠ“å–æ–¹æ³•ï¼ˆä¿æŒä¸å˜ï¼‰ ====================
    def fetch_arxiv(self, source):
        """æŠ“å–Arxiv AIè®ºæ–‡"""
        try:
            response = requests.get(source['url'], headers=self.headers, timeout=15)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                dt_list = soup.find_all('dt')
                dd_list = soup.find_all('dd')
                
                for i, (dt, dd) in enumerate(zip(dt_list[:8], dd_list[:8])):
                    paper_id_elem = dt.find('a', title='Abstract')
                    if not paper_id_elem:
                        continue
                    
                    paper_id = paper_id_elem.text.strip()
                    title_elem = dd.find('div', class_='list-title')
                    authors_elem = dd.find('div', class_='list-authors')
                    abstract_elem = dd.find('p')
                    
                    if title_elem:
                        title = title_elem.text.replace('Title:', '').strip()
                        authors = authors_elem.text.replace('Authors:', '').strip() if authors_elem else ''
                        abstract = abstract_elem.text.strip() if abstract_elem else ''
                        
                        article = {
                            'id': f"arxiv_{paper_id}",
                            'title': f"[è®ºæ–‡] {title[:120]}",
                            'link': f'https://arxiv.org/abs/{paper_id}',
                            'source': source['name'],
                            'summary': abstract[:200] + '...' if len(abstract) > 200 else abstract,
                            'authors': authors,
                            'category': 'research',
                            'importance': 9,
                            'time': datetime.now().strftime('%Y-%m-%d'),
                            'type': 'ai'
                        }
                        self.all_articles.append(article)
                        self.ai_articles.append(article)
        except Exception as e:
            print(f"âš ï¸ ArxivæŠ“å–å¤±è´¥: {e}")
    
    def fetch_rss(self, source, article_type='ai'):
        """é€šç”¨RSSæŠ“å–æ–¹æ³•ï¼Œå¢å¼ºå»é‡å’Œæ—¶æ•ˆæ€§"""
        try:
            feed = feedparser.parse(source['url'])
            articles_added = 0
            seen_links = set()  # å¢å¼ºå»é‡
            
            for entry in feed.entries[:20]:  # å¢åŠ æ£€æŸ¥èŒƒå›´ä»¥è·å–æ›´å¤šæ–°é²œå†…å®¹
                if articles_added >= 5:  # æ¯ä¸ªæºæœ€å¤šå–5æ¡
                    break
                    
                # æ£€æŸ¥å‘å¸ƒæ—¶é—´
                pub_time = None
                if hasattr(entry, 'published_parsed'):
                    pub_time = datetime(*entry.published_parsed[:6])
                elif hasattr(entry, 'updated_parsed'):
                    pub_time = datetime(*entry.updated_parsed[:6])
                
                # å¦‚æœæ— æ³•è·å–æ—¶é—´ï¼Œä½¿ç”¨å½“å‰æ—¶é—´ä½†é™ä½ä¼˜å…ˆçº§
                if not pub_time:
                    pub_time = datetime.now()
                    article_importance = 4  # é™ä½æœªçŸ¥æ—¶é—´çš„é‡è¦æ€§
                
                # æ£€æŸ¥æ˜¯å¦åœ¨è¿‡å»48å°æ—¶å†…
                if pub_time < self.forty_eight_hours_ago:
                    continue
                
                title = entry.get('title', '').strip()
                summary = entry.get('summary', '').strip()
                link = entry.get('link', '').strip()
                
                # å»é‡æ£€æŸ¥
                link_hash = hashlib.md5(link.encode()).hexdigest()
                if link_hash in seen_links:
                    continue
                seen_links.add(link_hash)
                
                # æ¸…ç†HTMLæ ‡ç­¾
                if summary:
                    soup = BeautifulSoup(summary, 'html.parser')
                    summary = soup.get_text()[:250]
                
                article = {
                    'id': link_hash[:8],
                    'title': title[:150],
                    'link': link,
                    'source': source['name'],
                    'summary': summary[:250] + '...' if len(summary) > 250 else summary,
                    'category': source.get('category', 'general'),
                    'lang': source.get('lang', 'en'),
                    'importance': 6,
                    'time': pub_time.strftime('%Y-%m-%d %H:%M'),
                    'type': article_type
                }
                
                # å¦‚æœæ˜¯è‹±æ–‡ï¼Œè¿›è¡Œç¿»è¯‘ä»¥æä¾›ä¸­è‹±æ–‡å¯¹ç…§
                if article['lang'] == 'en' and self.zhipu_api_key:
                    translated = self.translate_with_zhipu(title, summary)
                    if translated:
                        article['title_translated'] = translated['title']
                        article['summary_translated'] = translated['summary']
                
                # å¦‚æœæ˜¯AIæ–°é—»æºï¼Œæ£€æŸ¥æ˜¯å¦AIç›¸å…³
                if article_type == 'ai':
                    content = f"{title} {summary}".lower()
                    ai_keywords = ['ai', 'artificial intelligence', 'machine learning', 
                              'deep learning', 'neural network', 'llm', 'gpt', 'transformer',
                              'äººå·¥æ™ºèƒ½', 'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'å¤§æ¨¡å‹', 'ç”Ÿæˆå¼AI', 'è®¡ç®—æœºè§†è§‰', 'å›¾åƒç”Ÿæˆ','è®­ç»ƒ',
                              'AIGC', 'Diffusionæ¨¡å‹', 'MoEæ¨¡å‹', 'RLHF']
                    
                    is_ai_related = any(keyword in content for keyword in ai_keywords)
                    if is_ai_related:
                        article['importance'] = 8
                        self.all_articles.append(article)
                        self.ai_articles.append(article)
                        articles_added += 1
                else:
                    # äº‹å®æ–°é—»ç›´æ¥æ·»åŠ ï¼Œæ£€æŸ¥é‡å¤
                    if link_hash not in [a['id'] for a in self.fact_articles]:
                        self.all_articles.append(article)
                        self.fact_articles.append(article)
                        articles_added += 1
                    
        except Exception as e:
            print(f"âš ï¸ RSSæŠ“å–å¤±è´¥ {source['name']}: {e}")
    
    def fetch_hackernews(self, source, article_type='ai'):
        """é€šç”¨Hacker NewsæŠ“å–æ–¹æ³•"""
        try:
            timestamp = int(self.forty_eight_hours_ago.timestamp())
            query_param = source['url'].format(timestamp)
            url = query_param
            
            # å¦‚æœä¸æ˜¯AIä¸“ç”¨æœç´¢ï¼Œç§»é™¤AIæŸ¥è¯¢å‚æ•°
            if article_type == 'fact' and 'query=AI' in url:
                url = url.replace('&query=AI', '')
            
            response = requests.get(url, timeout=10)
            
            if response.status_code == 200:
                hits = response.json().get('hits', [])
                seen_links = set()
                for hit in hits[:10]:
                    link = hit.get('url', f"https://news.ycombinator.com/item?id={hit.get('objectID')}")
                    link_hash = hashlib.md5(link.encode()).hexdigest()
                    if link_hash in seen_links:
                        continue
                    seen_links.add(link_hash)
                    
                    title = hit.get('title', '')
                    
                    # å¯¹äºäº‹å®æ–°é—»ï¼Œä¸ç­›é€‰AIå†…å®¹
                    if article_type == 'ai' and not any(keyword in title.lower() for keyword in ['ai', 'llm', 'gpt', 'openai', 'anthropic']):
                        continue
                    
                    article = {
                        'id': f"hn_{hit.get('objectID', '')}",
                        'title': title,
                        'link': link,
                        'source': source['name'],
                        'points': hit.get('points', 0),
                        'comments': hit.get('num_comments', 0),
                        'category': source.get('category', 'tech'),
                        'importance': min(9, 6 + (hit.get('points', 0) // 20)),
                        'time': datetime.fromtimestamp(hit.get('created_at_i', 0)).strftime('%Y-%m-%d %H:%M'),
                        'type': article_type
                    }
                    
                    # ç¿»è¯‘å¦‚æœè‹±æ–‡
                    if source.get('lang') == 'en' and self.zhipu_api_key:
                        translated = self.translate_with_zhipu(title, '')
                        if translated:
                            article['title_translated'] = translated['title']
                    
                    self.all_articles.append(article)
                    if article_type == 'ai':
                        self.ai_articles.append(article)
                    else:
                        self.fact_articles.append(article)
                        
        except Exception as e:
            print(f"âš ï¸ Hacker NewsæŠ“å–å¤±è´¥: {e}")
    
    # ==================== æ–°å¢ï¼šç¿»è¯‘åŠŸèƒ½ ====================
    def translate_with_zhipu(self, title, summary):
        """ä½¿ç”¨æ™ºè°±AIç¿»è¯‘è‹±æ–‡åˆ°ä¸­æ–‡ï¼Œæä¾›è´´åˆå®é™…çš„ç¿»è¯‘"""
        try:
            from zhipuai import ZhipuAI
            
            client = ZhipuAI(api_key=self.zhipu_api_key)
            
            prompt = f"""ä½œä¸ºä¸“ä¸šç¿»è¯‘ï¼Œè¯·å°†ä»¥ä¸‹è‹±æ–‡å†…å®¹ç¿»è¯‘æˆè´´åˆå®é™…ã€è‡ªç„¶æµç•…çš„ä¸­æ–‡ï¼š
æ ‡é¢˜ï¼š{title}
æ‘˜è¦ï¼š{summary}

è¯·æä¾›ä¸­è‹±æ–‡å¯¹ç…§ï¼š
- åŸæ ‡é¢˜ï¼š[original title]
- ç¿»è¯‘æ ‡é¢˜ï¼š[translated title]
- åŸæ‘˜è¦ï¼š[original summary]
- ç¿»è¯‘æ‘˜è¦ï¼š[translated summary]

è¾“å‡ºJSONæ ¼å¼ï¼š
{{
  "title": "translated title",
  "summary": "translated summary"
}}
ä½†åœ¨æŠ¥å‘Šä¸­å¯æ˜¾ç¤ºå®Œæ•´å¯¹ç…§ã€‚
"""
            
            response = client.chat.completions.create(
                model="glm-3-turbo",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è‹±ä¸­ç¿»è¯‘ä¸“å®¶ï¼Œç¿»è¯‘è¦å‡†ç¡®ã€è‡ªç„¶ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2,
                max_tokens=400
            )
            
            result_text = response.choices[0].message.content
            json_match = re.search(r'\{.*\}', result_text, re.DOTALL)
            
            if json_match:
                return json.loads(json_match.group())
            else:
                return None
            
        except Exception as e:
            print(f"âš ï¸ ç¿»è¯‘å¤±è´¥: {e}")
            return None
    
    # ==================== æ–°å¢ï¼šæŠ“å–äº‹å®æ–°é—» ====================
    def fetch_fact_news(self):
        """æŠ“å–å¤šæ–¹é¢äº‹å®æ–°é—»"""
        print("\nğŸ“° å¼€å§‹æŠ“å–å¤šæ–¹é¢äº‹å®æ–°é—»ï¼ˆè¿‡å»48å°æ—¶ï¼‰...")
        
        for source in self.fact_news_sources:
            print(f"  â†’ {source['name']}")
            try:
                if source['type'] == 'rss':
                    self.fetch_rss(source, article_type='fact')
                elif source['type'] == 'hn_api':
                    self.fetch_hackernews(source, article_type='fact')
                time.sleep(1)  # ç¤¼è²Œå»¶è¿Ÿ
            except Exception as e:
                print(f"    âŒ æŠ“å–å¤±è´¥: {e}")
                continue
        
        print(f"âœ… äº‹å®æ–°é—»æŠ“å–å®Œæˆï¼å…±è·å¾— {len(self.fact_articles)} ç¯‡")
        
        # å»é‡
        unique_facts = []
        seen_ids = set()
        for article in self.fact_articles:
            if article['id'] not in seen_ids:
                unique_facts.append(article)
                seen_ids.add(article['id'])
        
        # æ’åºï¼šä¼˜å…ˆçº§é«˜ â†’ é‡è¦æ€§é«˜ â†’ æ—¶é—´æ–°
        self.fact_articles = sorted(
            unique_facts,
            key=lambda x: (
                -x.get('priority', 5),                     # æ³¨æ„è´Ÿå·ï¼šè¶Šé«˜ä¼˜å…ˆçº§è¶Šé å‰
                x.get('importance', 5),
                datetime.strptime(x['time'], '%Y-%m-%d %H:%M') if x.get('time') else datetime.now()
            ),
            reverse=True
        )[:12]  # æœ€å¤šä¿ç•™12æ¡
    
    # ==================== åŸæœ‰AIåˆ†æåŠŸèƒ½ï¼ˆä¿æŒä¸å˜ï¼‰ ====================
    def fetch_all_news(self):
        """æŠ“å–æ‰€æœ‰æ–°é—»"""
        print("ğŸ“¡ å¼€å§‹æŠ“å–AIç§‘æŠ€æ–°é—»ï¼ˆè¿‡å»48å°æ—¶ï¼‰...")
        for source in self.ai_news_sources:
            print(f"  â†’ {source['name']}")
            try:
                if source['type'] == 'arxiv':
                    self.fetch_arxiv(source)
                elif source['type'] == 'rss':
                    self.fetch_rss(source, article_type='ai')
                elif source['type'] == 'hn_api':
                    self.fetch_hackernews(source, article_type='ai')
                time.sleep(1)
            except Exception as e:
                print(f"    âŒ æŠ“å–å¤±è´¥: {e}")
        
        print(f"âœ… AIæ–°é—»æŠ“å–å®Œæˆï¼å…±è·å¾— {len(self.ai_articles)} ç¯‡")
    
    def analyze_with_zhipu(self, article):
        """ä½¿ç”¨æ™ºè°±AIåˆ†ææ–‡ç« """
        try:
            from zhipuai import ZhipuAI
            
            client = ZhipuAI(api_key=self.zhipu_api_key)
            
            prompt = f"""ä½œä¸ºæ–°é—»åˆ†æå¸ˆï¼Œè¯·åˆ†æä»¥ä¸‹æ–‡ç« ï¼š

æ ‡é¢˜ï¼š{article['title']}
æ¥æºï¼š{article['source']}
æ‘˜è¦ï¼š{article.get('summary', 'æš‚æ— è¯¦ç»†æ‘˜è¦')}

è¯·æä¾›ä»¥ä¸‹åˆ†æï¼š
1. æ ¸å¿ƒå†…å®¹è¦ç‚¹
2. æ–°é—»é‡è¦æ€§ï¼ˆé«˜/ä¸­/ä½ï¼‰
3. å½±å“èŒƒå›´ï¼ˆå›½é™…/å›½å†…/åŒºåŸŸ/è¡Œä¸šï¼‰
4. å€¼å¾—å…³æ³¨çš„ç†ç”±
5. å†…å®¹æ ‡ç­¾ï¼ˆ3-5ä¸ªå…³é”®è¯ï¼‰

è¯·ç”¨JSONæ ¼å¼å›å¤ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- key_points: åˆ—è¡¨ï¼Œæ ¸å¿ƒå†…å®¹è¦ç‚¹
- importance_level: å­—ç¬¦ä¸²ï¼Œé«˜/ä¸­/ä½
- impact_scope: å­—ç¬¦ä¸²
- attention_reason: å­—ç¬¦ä¸²
- content_tags: åˆ—è¡¨ï¼Œå†…å®¹æ ‡ç­¾
"""
            
            response = client.chat.completions.create(
                model="glm-3-turbo",  # ä½¿ç”¨æ€§ä»·æ¯”æ›´é«˜çš„æ¨¡å‹
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–°é—»åˆ†æå¸ˆã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=600
            )
            
            result_text = response.choices[0].message.content
            json_match = re.search(r'\{.*\}', result_text, re.DOTALL)
            
            if json_match:
                analysis_result = json.loads(json_match.group())
            else:
                analysis_result = {
                    "key_points": ["é‡è¦æ–°é—»"],
                    "importance_level": "ä¸­",
                    "impact_scope": "å¹¿æ³›å…³æ³¨",
                    "attention_reason": "å€¼å¾—å…³æ³¨çš„æ–°é—»æŠ¥é“",
                    "content_tags": ["æ–°é—»"]
                }
            
            return {
                'content_tags': analysis_result.get('content_tags', ['æ–°é—»']),
                'importance_level': analysis_result.get('importance_level', 'ä¸­'),
                'impact_scope': analysis_result.get('impact_scope', 'å¹¿æ³›'),
                'attention_reason': analysis_result.get('attention_reason', 'å€¼å¾—å…³æ³¨'),
                'key_points': analysis_result.get('key_points', []),
                'source': 'zhipu_ai'
            }
            
        except Exception as e:
            print(f"âš ï¸ æ™ºè°±AIåˆ†æå¤±è´¥: {e}")
            return self._fallback_analysis(article)
    
    def _fallback_analysis(self, article):
        """å¤‡ç”¨å…³é”®è¯åˆ†æ"""
        text = f"{article['title']} {article.get('summary', '')}".lower()
        
        # æ ¹æ®å†…å®¹åˆ¤æ–­ç±»åˆ«
        tags = []
        if any(word in text for word in ['politics', 'government', 'policy', 'æ”¿æ²»', 'æ”¿åºœ']):
            tags.append('æ”¿æ²»')
        if any(word in text for word in ['economy', 'financial', 'market', 'ç»æµ', 'é‡‘è']):
            tags.append('ç»æµ')
        if any(word in text for word in ['technology', 'tech', 'digital', 'ç§‘æŠ€', 'æŠ€æœ¯']):
            tags.append('ç§‘æŠ€')
        if any(word in text for word in ['health', 'medical', 'ç–«æƒ…', 'ç–«è‹—', 'å¥åº·']):
            tags.append('å¥åº·')
        if any(word in text for word in ['environment', 'climate', 'ç¯ä¿', 'æ°”å€™']):
            tags.append('ç¯å¢ƒ')
        if not tags:
            tags = ['ç»¼åˆæ–°é—»']
        
        return {
            'content_tags': tags,
            'importance_level': 'ä¸­',
            'impact_scope': 'å¹¿æ³›å…³æ³¨',
            'attention_reason': 'å€¼å¾—å…³æ³¨çš„æ–°é—»æŠ¥é“',
            'key_points': tags,
            'source': 'keyword_analysis'
        }
    
    def generate_deep_analyses(self, limit=3):
        """ç”Ÿæˆæ·±åº¦åˆ†æï¼ˆAIæ–°é—»ï¼‰"""
        if not self.ai_articles:
            return []
        
        important_articles = sorted(
            self.ai_articles,
            key=lambda x: x.get('importance', 5),
            reverse=True
        )[:limit]
        
        print(f"\nğŸ” å¼€å§‹æ·±åº¦åˆ†æ {len(important_articles)} ç¯‡AIæ–‡ç« ...")
        
        analyses = []
        for i, article in enumerate(important_articles, 1):
            print(f"  {i}. åˆ†æ: {article['title'][:60]}...")
            analysis = self.analyze_with_zhipu(article)
            
            # å¦‚æœæœ‰ç¿»è¯‘ï¼Œä½¿ç”¨ç¿»è¯‘
            title_display = article.get('title_translated', article['title'])
            
            analysis_text = f"""### ğŸ“‘ è®ºæ–‡ {title_display}

**æ¥æº**: {article['source']} | **æ—¶é—´**: {article.get('time', 'N/A')} | **AIåˆ†ææ¨¡å‹**: ğŸ¤– æ™ºè°±GLM

**åŸæ–‡é“¾æ¥**: {article['link']}

**å†…å®¹æ‘˜è¦**:
{analysis.get('content_summary', 'æš‚æ— æ‘˜è¦')}

**å†…å®¹æ ‡ç­¾**: {', '.join(analysis.get('content_tags', []))}

**é‡è¦æ€§**: {analysis.get('importance_level', 'ä¸­')}

**å½±å“èŒƒå›´**: {analysis.get('impact_scope', 'å¹¿æ³›å…³æ³¨')}

**å…³æ³¨ç†ç”±**: {analysis.get('attention_reason', 'å€¼å¾—å…³æ³¨çš„æŠ¥é“')}

**æ ¸å¿ƒè¦ç‚¹**ï¼ˆæ ‡ç­¾å½¢å¼ï¼‰:
{chr(10).join(f'- {point}' for point in analysis.get('key_points', []))}

---
"""
            analyses.append({
                'article': article,
                'analysis': analysis,
                'text': analysis_text
            })
            
            if self.zhipu_api_key:
                time.sleep(1)  # APIè°ƒç”¨é—´éš”
        
        self.deep_analyses = analyses
        return analyses
    
    def select_featured_articles(self):
        """é€‰æ‹©ç²¾é€‰æ–‡ç« """
        if self.ai_articles:
            scored_ai = sorted(
                [(a.get('importance', 5), a) for a in self.ai_articles],
                reverse=True, key=lambda x: x[0]
            )
            if scored_ai:
                self.featured_article = scored_ai[0][1]
        
        if self.fact_articles:
            # äº‹å®æ–°é—»æŒ‰é‡è¦æ€§å’Œæ—¶æ•ˆæ€§è¯„åˆ†
            for article in self.fact_articles:
                # åŠ åˆ†é¡¹ï¼šé«˜é‡è¦æ€§ã€å¤šè¯„è®º/åˆ†æ•°ã€è¿‘æœŸå‘å¸ƒ
                score = article.get('importance', 5)
                if article.get('points', 0) > 50:
                    score += 1
                if article.get('comments', 0) > 20:
                    score += 1
                article['_score'] = score
            
            scored_facts = sorted(
                self.fact_articles,
                key=lambda x: x.get('_score', 5),
                reverse=True
            )
            if scored_facts:
                self.featured_fact = scored_facts[0]
                self.featured_fact['generated_summary'] = self.generate_fact_summary(self.featured_fact)
    
    def generate_fact_summary(self, article):
        """ä¸ºäº‹å®ç²¾é€‰ç”Ÿæˆç®€çŸ­æ‘˜è¦"""
        if not self.zhipu_api_key:
            return article.get('summary_translated', article.get('summary', 'æš‚æ— æ‘˜è¦'))[:100] + '...'
        
        try:
            from zhipuai import ZhipuAI
            client = ZhipuAI(api_key=self.zhipu_api_key)
            
            prompt = f"""åŸºäºä»¥ä¸‹æ–°é—»æ ‡é¢˜å’Œé“¾æ¥ï¼Œç”Ÿæˆ80-120å­—ä¸­æ–‡æ‘˜è¦ï¼š
æ ‡é¢˜ï¼š{article['title']}
é“¾æ¥ï¼š{article['link']}

æ‘˜è¦è¦æ±‚ï¼šæç‚¼æ ¸å¿ƒäº‹ä»¶/å†…å®¹/æ•°æ®/æ„ä¹‰ï¼Œè¯­è¨€å®¢è§‚ä¸“ä¸šã€‚"""
            
            response = client.chat.completions.create(
                model="glm-3-turbo",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=150
            )
            
            summary = response.choices[0].message.content.strip()
            if len(summary) > 120:
                summary = summary[:117] + "..."
            return summary
        except Exception as e:
            print(f"âš ï¸ ç”Ÿæˆæ‘˜è¦å¤±è´¥: {e}")
            return article.get('summary_translated', article.get('summary', 'æš‚æ— æ‘˜è¦'))[:100] + '...'
    
    def format_fact_news_section(self):
        """æ•´ç†äº‹å®æ–°é—»éƒ¨åˆ†ï¼Œåˆ†ç»„æ˜¾ç¤ºå›½å†…+å›½é™…"""
        if not self.fact_articles:
            return ""

        section = f"""
## ğŸŒ 48å°æ—¶äº‹å®èµ„è®¯é€Ÿè§ˆ ({len(self.fact_articles)}ç¯‡)

*äº‹å®æ–°é—»æ¥è‡ª {len(set([a['source'] for a in self.fact_articles]))} ä¸ªå›½å†…å¤–æƒå¨åª’ä½“*
*ç­›é€‰è¿‡å»48å°æ—¶æœ€é‡è¦æ–°é—»ï¼Œä¿æŒä¿¡æ¯å¹¿åº¦ä¸æ·±åº¦*
"""

        # â”€â”€ å›½å†…æ–°é—» â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        domestic = [
            a for a in self.fact_articles 
            if a.get('lang') == 'zh' or a.get('category') in ['china', 'cn']
        ]
        domestic = sorted(domestic, key=lambda x: x.get('importance', 5), reverse=True)[:7]

        if domestic:
            section += f"""
### ğŸ‡¨ğŸ‡³ å›½å†…æ–°é—»
"""
            for i, article in enumerate(domestic, 1):
                title_orig = article['title']
                title_cn = article.get('title_translated', title_orig)
                source = article['source']
                link = article['link']

                section += f"{i}. **{title_orig}**\n"
                if title_cn != title_orig:
                    section += f"   {title_cn}\n"
                section += f"   ğŸ“ {source} | ğŸ”— [é˜…è¯»åŸæ–‡]({link})\n\n"

        # â”€â”€ å›½é™…æ–°é—» â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        international = [
            a for a in self.fact_articles 
            if a.get('lang') != 'zh' or a.get('category') in ['world', 'asia', 'international']
        ]
        international = sorted(international, key=lambda x: x.get('importance', 5), reverse=True)[:7]

        if international:
            section += f"""
### ğŸŒ å›½é™…æ–°é—»
"""
            for i, article in enumerate(international, 1):
                title_orig = article['title']
                title_cn = article.get('title_translated', title_orig)
                source = article['source']
                link = article['link']

                section += f"{i}. **{title_orig}**\n"
                if title_cn != title_orig:
                    section += f"   {title_cn}\n"
                section += f"   ğŸ“ {source} | ğŸ”— [é˜…è¯»åŸæ–‡]({link})\n\n"

        # â”€â”€ ä»Šæ—¥äº‹å®ç²¾é€‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if self.featured_fact:
            featured = self.featured_fact
            title_orig = featured['title']
            title_cn = featured.get('title_translated', title_orig)

            summary_text = featured.get('generated_summary',
                                       featured.get('summary_translated',
                                                   featured.get('summary', 'æš‚æ— å¯ç”¨æ‘˜è¦')))

            if len(summary_text) > 120:
                summary_text = summary_text[:117] + "â€¦"

            section += f"""
## ğŸ“° ä»Šæ—¥äº‹å®ç²¾é€‰

**{title_orig}**  
{title_cn if title_cn != title_orig else ''}

**æ¥æº**ï¼š{featured['source']} | **æ—¶é—´**ï¼š{featured.get('time', 'ä»Šæ—¥')}

**æ‘˜è¦**ï¼š{summary_text}

**æ·±åº¦é˜…è¯»**ï¼š{featured['link']}
"""

        return section
    
    def generate_report(self):
        """ç”Ÿæˆå®Œæ•´æŠ¥å‘Š"""
        current_time = datetime.now().strftime('%Y-%m-%d %H:%M')
        
        report = f"""# ğŸ“Š æ¯æ—¥èµ„è®¯åŒæŠ¥å‘Š ({current_time})

## ğŸ“ˆ æ•°æ®æ€»è§ˆ
- **AIç§‘æŠ€èµ„è®¯**: {len(self.ai_articles)} ç¯‡
- **äº‹å®èµ„è®¯**: {len(self.fact_articles)} ç¯‡
- **æ·±åº¦åˆ†æ**: {len(self.deep_analyses)} ç¯‡
- **è¦†ç›–åª’ä½“**: {len(self.ai_news_sources) + len(self.fact_news_sources)} ä¸ª

"""
        
        # 1. AIç§‘æŠ€æ–°é—»éƒ¨åˆ†
        if self.ai_articles:
            report += f"""
## ğŸ¤– AIç§‘æŠ€æ—¥æŠ¥

### ğŸš€ AIå¿«è®¯æ‘˜è¦
"""
            ai_by_category = {}
            for article in self.ai_articles[:15]:
                cat = article.get('category', 'other')
                if cat not in ai_by_category:
                    ai_by_category[cat] = []
                ai_by_category[cat].append(article)
            
            category_names = {
                'research': 'ğŸ§ª ç ”ç©¶å‰æ²¿',
                'tech': 'ğŸ”§ æŠ€æœ¯åŠ¨æ€',
                'community': 'ğŸ‘¥ ç¤¾åŒºçƒ­ç‚¹',
                'cn_ai': 'ğŸ‡¨ğŸ‡³ å›½å†…AI'
            }
            
            for cat, articles in ai_by_category.items():
                name = category_names.get(cat, 'ğŸ“Œ å…¶ä»–')
                report += f"\n**{name}**\n"
                for i, article in enumerate(articles[:3], 1):
                    title_display = article.get('title_translated', article['title'])
                    report += f"{i}. {title_display}\n"
                    report += f"   ğŸ“ {article['source']} | ğŸ”— [é˜…è¯»åŸæ–‡]({article['link']})\n"
            
            # AIæ·±åº¦åˆ†æ
            if self.deep_analyses:
                report += "\n## ğŸ” AIæ·±åº¦åˆ†æ\n"
                report += "_ä»¥ä¸‹AIæ–‡ç« å·²è¿›è¡Œè¯¦ç»†æŠ€æœ¯åˆ†æï¼š_\n\n"
                for analysis in self.deep_analyses:
                    report += analysis['text']
            
            # AIç²¾é€‰
            if self.featured_article:
                featured_title = self.featured_article.get('title_translated', self.featured_article['title'])
                featured_summary = self.featured_article.get('summary_translated', self.featured_article.get('summary', 'æš‚æ— æ‘˜è¦'))
                
                report += f"""
## ğŸ† ä»Šæ—¥AIç²¾é€‰

**{featured_title}**

**æ¥æº**: {self.featured_article['source']}
**æ‘˜è¦**: {featured_summary}

**ğŸ”— æ·±åº¦é˜…è¯»**: {self.featured_article['link']}
"""
        
        # 2. äº‹å®æ–°é—»éƒ¨åˆ†
        report += self.format_fact_news_section()
        
        # 3. æ€»ç»“
        report += f"""

---

## ğŸ“‹ æŠ¥å‘Šä¿¡æ¯
- **ç”Ÿæˆæ—¶é—´**: {current_time}
- **ä¸‹æ¬¡æ›´æ–°**: æ˜æ—¥ 08:00 (åŒ—äº¬æ—¶é—´)
- **åˆ†ææ”¯æŒ**: æ™ºè°±AI GLMæ¨¡å‹
- **æ¨é€æ–¹å¼**: Serveré…±å¾®ä¿¡æ¨é€

*ä¿æŒä¿¡æ¯æ•æ„Ÿåº¦ï¼Œæ‹¥æŠ±ç§‘æŠ€å˜é©ï¼Œå…³æ³¨ä¸–ç•ŒåŠ¨æ€*
"""
        
        title = f"èµ„è®¯åŒæŠ¥å‘Š {datetime.now().strftime('%m-%d')} | AI:{len(self.ai_articles)} äº‹å®:{len(self.fact_articles)}"
        
        return report, title
    
    def save_reports(self, report):
        """ä¿å­˜æŠ¥å‘Š"""
        output_data = {
            'fetch_time': datetime.now().isoformat(),
            'ai_articles_count': len(self.ai_articles),
            'fact_articles_count': len(self.fact_articles),
            'deep_analyses_count': len(self.deep_analyses),
            'featured_article': self.featured_article,
            'featured_fact': self.featured_fact,
            'ai_articles': self.ai_articles[:20],
            'fact_articles': self.fact_articles[:10]
        }
        
        with open('enhanced_news_analysis.json', 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        with open('enhanced_news_report.md', 'w', encoding='utf-8') as f:
            f.write(report)
        
        print("ğŸ’¾ æŠ¥å‘Šå·²ä¿å­˜è‡³: enhanced_news_analysis.json, enhanced_news_report.md")
    
    def send_to_wechat(self, report):
        """é€šè¿‡Serveré…±å‘é€åˆ°å¾®ä¿¡"""
        if not self.server_chan_key:
            print("âš ï¸ æœªé…ç½®Serveré…±å¯†é’¥ï¼Œè·³è¿‡æ¨é€")
            return False
        
        url = f"https://sctapi.ftqq.com/{self.server_chan_key}.send"
        
        if len(report) > 6000:
            report = report[:6000] + "\n\n...ï¼ˆæŠ¥å‘Šè¿‡é•¿ï¼Œå·²æˆªæ–­ï¼Œå®Œæ•´å†…å®¹è¯·æŸ¥çœ‹ä¿å­˜çš„æ–‡ä»¶ï¼‰"
        
        data = {
            'title': f"èµ„è®¯åŒæŠ¥å‘Š {datetime.now().strftime('%m-%d')} | AI:{len(self.ai_articles)} äº‹å®:{len(self.fact_articles)}",
            'desp': report
        }
        
        try:
            response = requests.post(url, data=data, timeout=15)
            result = response.json()
            
            if result.get('code') == 0:
                print(f"âœ… å¾®ä¿¡æ¨é€æˆåŠŸï¼æ¶ˆæ¯ID: {result.get('data', {}).get('pushid')}")
                return True
            else:
                print(f"âŒ æ¨é€å¤±è´¥: {result}")
                return False
        except Exception as e:
            print(f"âŒ æ¨é€è¯·æ±‚å¤±è´¥: {e}")
            return False
    
        def run(self):
        """ä¸»æ‰§è¡Œå‡½æ•°"""
        print("=" * 70)
        print("ğŸ“Š å¢å¼ºç‰ˆèµ„è®¯åˆ†æç³»ç»Ÿå¯åŠ¨")
        print(f"ğŸ“… æ‰§è¡Œæ—¶é—´: {datetime.now()}")
        print("=" * 70)
        
        # 1. æŠ“å–AIæ–°é—»
        self.fetch_all_news()
        
        # 2. æŠ“å–äº‹å®æ–°é—»ï¼ˆæ’åºå·²ç§»åˆ° fetch_fact_news å…§ï¼‰
        self.fetch_fact_news()
        
        if not self.all_articles:
            print("âŒ æœªæŠ“å–åˆ°ä»»ä½•æ–‡ç« ï¼Œç¨‹åºé€€å‡º")
            return None, "æ— å†…å®¹"
        
        # 3. ç”ŸæˆAIæ·±åº¦åˆ†æ
        self.generate_deep_analyses(limit=3)
        
        # 4. é€‰æ‹©ç²¾é€‰æ–‡ç« 
        self.select_featured_articles()
        
        # 5. ç”ŸæˆæŠ¥å‘Š
        report, title = self.generate_report()
        
        # 6. ä¿å­˜æŠ¥å‘Š
        self.save_reports(report)
        
        print(f"\nğŸ“Š æŠ¥å‘Šç”Ÿæˆå®Œæˆ:")
        print(f"   AIèµ„è®¯: {len(self.ai_articles)} ç¯‡")
        print(f"   äº‹å®èµ„è®¯: {len(self.fact_articles)} ç¯‡")
        print(f"   æŠ¥å‘Šæ ‡é¢˜: {title}")
        
        return report, title
        
        # 3. ç”ŸæˆAIæ·±åº¦åˆ†æ
        self.generate_deep_analyses(limit=3)
        
        # 4. é€‰æ‹©ç²¾é€‰æ–‡ç« 
        self.select_featured_articles()
        
        # 5. ç”ŸæˆæŠ¥å‘Š
        report, title = self.generate_report()
        
        # 6. ä¿å­˜æŠ¥å‘Š
        self.save_reports(report)
        
        print(f"\nğŸ“Š æŠ¥å‘Šç”Ÿæˆå®Œæˆ:")
        print(f"   AIèµ„è®¯: {len(self.ai_articles)} ç¯‡")
        print(f"   äº‹å®èµ„è®¯: {len(self.fact_articles)} ç¯‡")
        print(f"   æŠ¥å‘Šæ ‡é¢˜: {title}")
        
        return report, title

def main():
    analyzer = EnhancedNewsAnalyzer()
    report, title = analyzer.run()
    
    if report:
        if analyzer.server_chan_key:
            print("\nğŸ“¤ æ­£åœ¨å‘é€åˆ°å¾®ä¿¡...")
            analyzer.send_to_wechat(report)
        else:
            print("\nâš ï¸ æœªé…ç½®SERVER_CHAN_KEYï¼Œè·³è¿‡æ¨é€")
        
        # æ‰“å°é¢„è§ˆ
        print("\n" + "=" * 70)
        print("ğŸ“‹ å†…å®¹é¢„è§ˆ:")
        print("=" * 70)
        preview_length = min(2000, len(report))
        print(report[:preview_length] + "..." if len(report) > preview_length else report)
    else:
        print("âŒ æœªç”ŸæˆæŠ¥å‘Šï¼Œè¯·æ£€æŸ¥é…ç½®")

if __name__ == "__main__":
    main()
