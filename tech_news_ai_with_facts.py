#!/usr/bin/env python3
"""
AIç§‘æŠ€èµ„è®¯ä¸äº‹å®èµ„è®¯æ™ºèƒ½åˆ†æç³»ç»Ÿ
æŠ“å–è¿‡å»48å°æ—¶AI/ç§‘æŠ€èµ„è®¯å’Œå¤šæ–¹äº‹å®æ–°é—»ï¼Œæ™ºèƒ½åˆ†æåæ¨é€
"""

import os
import re
import json
import requests
import hashlib
from datetime import datetime, timedelta
import time
from bs4 import BeautifulSoup
import feedparser
from urllib.parse import urljoin
from collections import Counter
import random  # ç”¨äºç”Ÿæˆ salt
import google.generativeai as genai
from bs4 import BeautifulSoup

# æ¡ä»¶å¯¼å…¥å¼‚æ­¥åº“ï¼ˆæä¾›å‹å¥½çš„é”™è¯¯æç¤ºï¼‰
try:
    import asyncio
    import aiohttp
    ASYNC_AVAILABLE = True
except ImportError as e:
    print("âš ï¸  è­¦å‘Š: æœªå®‰è£…å¼‚æ­¥åº“ï¼Œå¼‚æ­¥åŠŸèƒ½å°†ä¸å¯ç”¨")
    print(f"   ç¼ºå¤±æ¨¡å—: {e.name}")
    print("   å®‰è£…å‘½ä»¤: pip install aiohttp")
    ASYNC_AVAILABLE = False
    # åˆ›å»ºå ä½ç¬¦é¿å…åç»­ä»£ç æŠ¥é”™
    asyncio = None
    aiohttp = None

class EnhancedNewsAnalyzer:
    def __init__(self):
        self.server_chan_key = os.getenv('SERVER_CHAN_KEY')
        self.gemini_api_key = os.getenv('GEMINI_API_KEY')
        self.forty_eight_hours_ago = datetime.now() - timedelta(hours=48)
        
        # æ‘˜è¦ç¼“å­˜å­—å…¸ï¼ˆé¿å…é‡å¤è¯·æ±‚åŒä¸€ç¯‡è®ºæ–‡ï¼‰
        self.abstract_cache = {}
        
        # é˜²å¾¡æ€§æ£€æŸ¥ï¼šAPIå¯†é’¥é…ç½®æé†’
        if not self.gemini_api_key:
            print("âš ï¸  è­¦å‘Š: æœªé…ç½® GEMINI_API_KEY ç¯å¢ƒå˜é‡")
            print("   â†’ æ·±åº¦åˆ†æå°†ä½¿ç”¨å¤‡ç”¨å…³é”®è¯åˆ†æï¼ˆåŠŸèƒ½å—é™ï¼‰")
            print("   â†’ å»ºè®®é…ç½® Gemini API ä»¥è·å¾—å®Œæ•´åˆ†æèƒ½åŠ›\n")
        
        if not self.server_chan_key:
            print("âš ï¸  è­¦å‘Š: æœªé…ç½® SERVER_CHAN_KEY ç¯å¢ƒå˜é‡")
            print("   â†’ å¾®ä¿¡æ¨é€åŠŸèƒ½å°†è¢«ç¦ç”¨\n")
        
        baidu_appid = os.getenv('BAIDU_APPID')
        baidu_secret_key = os.getenv('BAIDU_SECRET_KEY')
        if not baidu_appid or not baidu_secret_key:
            print("âš ï¸  è­¦å‘Š: æœªé…ç½®ç™¾åº¦ç¿»è¯‘ APIï¼ˆBAIDU_APPID/BAIDU_SECRET_KEYï¼‰")
            print("   â†’ è‹±æ–‡å†…å®¹å°†ä¸ä¼šè¢«ç¿»è¯‘ä¸ºä¸­æ–‡\n")
        
        # AIç§‘æŠ€æ–°é—»æºï¼ˆä¿æŒä¸å˜ï¼‰
        self.ai_news_sources = [
            {'name': 'Arxiv AI Papers', 'url': 'http://arxiv.org/list/cs.AI/recent', 'type': 'arxiv', 'category': 'ai_research'},
            {'name': 'TechCrunch AI', 'url': 'https://techcrunch.com/category/artificial-intelligence/feed/', 'type': 'rss', 'category': 'tech'},
            {'name': 'Hacker News AI', 'url': 'https://hn.algolia.com/api/v1/search_by_date?tags=story&numericFilters=created_at_i>{}&query=AI', 'type': 'hn_api', 'category': 'community'},
            {'name': 'æœºå™¨ä¹‹å¿ƒ', 'url': 'https://www.jiqizhixin.com/feed', 'type': 'rss', 'category': 'cn_ai'},
            {'name': 'é‡å­ä½', 'url': 'https://www.qbitai.com/feed', 'type': 'rss', 'category': 'cn_ai'},
        ]
        
        # æ›´æ–°ï¼šå¤šæ–¹é¢äº‹å®æ–°é—»æºï¼Œä¼˜å…ˆä¸­å›½å›½å†…å¯è®¿é—®æ¥æºï¼Œå‡å°‘é‡å¤å’Œè¿‡æ—¶
        self.fact_news_sources = [
            # å›½å†…æ–°é—»ï¼ˆä¼˜å…ˆå¯è®¿é—®æ¥æºï¼‰
            {'name': 'å¤®è§†ç½‘', 'url': 'http://news.cctv.com/rss/index.xml', 'type': 'rss', 'category': 'china', 'lang': 'zh'},
            {'name': 'æ–°åç½‘', 'url': 'http://www.news.cn/rss/rsstw.xml', 'type': 'rss', 'category': 'china', 'lang': 'zh'},  # æ›´æ–°ä¸ºæ›´ç¨³å®šçš„æ–°åç½‘RSS
            {'name': 'äººæ°‘æ—¥æŠ¥', 'url': 'http://www.people.com.cn/rss/politics.xml', 'type': 'rss', 'category': 'china', 'lang': 'zh'},
            {'name': 'æ¾æ¹ƒæ–°é—»', 'url': 'https://rsshub.app/thepaper/featured', 'type': 'rss', 'category': 'china', 'lang': 'zh'},
            {'name': 'è™æ‰‘ç¤¾åŒº', 'url': 'https://rsshub.app/hupu/bbs/all', 'type': 'rss', 'category': 'community', 'lang': 'zh'},  # æ·»åŠ è™æ‰‘ via RSSHub
            {'name': 'è…¾è®¯æ–°é—»', 'url': 'https://rsshub.app/tencent/news/author/1', 'type': 'rss', 'category': 'china', 'lang': 'zh'},  # æ·»åŠ è…¾è®¯æ–°é—»
            # å›½é™…/äºšå¤ªæ–°é—»ï¼ˆé€‰æ‹©åœ¨ä¸­å›½å¯è®¿é—®æˆ–ä¸­ç«‹æ¥æºï¼‰
            {'name': 'è”åˆæ—©æŠ¥', 'url': 'https://www.zaobao.com/realtime/china/rss', 'type': 'rss', 'category': 'asia', 'lang': 'zh'},  # æ›´æ–°ä¸ºä¸­å›½å®æ—¶
            {'name': 'BBCä¸­æ–‡', 'url': 'https://feeds.bbci.co.uk/zhongwen/simp/rss.xml', 'type': 'rss', 'category': 'world', 'lang': 'zh'},  # BBCä¸­æ–‡ç‰ˆï¼Œå¯è®¿é—®
            {'name': 'Reuters China', 'url': 'https://www.reuters.com/arc/outboundfeeds/rss/world/china/', 'type': 'rss', 'category': 'world', 'lang': 'en'},  # Reutersä¸­å›½ç›¸å…³
            # ç¤¾åŒº/ç»¼åˆ
            {'name': 'Reddit World News', 'url': 'https://www.reddit.com/r/worldnews/.rss', 'type': 'rss', 'category': 'world', 'lang': 'en'},
            {'name': 'Hacker News Top', 'url': 'https://hn.algolia.com/api/v1/search_by_date?tags=story&numericFilters=created_at_i>{}', 'type': 'hn_api', 'category': 'tech', 'lang': 'en'},
        ]
        
        self.all_articles = []
        self.ai_articles = []
        self.fact_articles = []
        self.deep_analyses = []
        self.featured_article = None
        self.featured_fact = None
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
    
    # ==================== æ–°å¢ï¼šç™¾åº¦ç¿»è¯‘å‡½æ•° ====================
    def baidu_translate(self, title, summary):
        """ä½¿ç”¨ç™¾åº¦APIç¿»è¯‘è‹±æ–‡åˆ°ä¸­æ–‡ï¼Œæä¾›è´´åˆå®é™…çš„ç¿»è¯‘ï¼Œå¤±è´¥æ—¶è¿”å›æ ‡è®°"""
        appid = os.getenv('BAIDU_APPID')
        secret_key = os.getenv('BAIDU_SECRET_KEY')
        if not appid or not secret_key:
            print("âš ï¸ æœªé…ç½®ç™¾åº¦ç¿»è¯‘å¯†é’¥ï¼Œè·³è¿‡ç¿»è¯‘")
            # è¿”å›ç‰¹æ®Šæ ‡è®°è¡¨ç¤ºæœªç¿»è¯‘
            return {
                'title': f"{title} (æœªç¿»è¯‘)",
                'summary': f"{summary} (æœªç¿»è¯‘)" if summary else "(æœªç¿»è¯‘)"
            }
        
        try:
            query = title + '\n' + summary if summary else title
            from_lang = 'en'
            to_lang = 'zh'
            salt = str(random.randint(32768, 65536))
            sign = hashlib.md5((appid + query + salt + secret_key).encode('utf-8')).hexdigest()
            
            url = 'http://api.fanyi.baidu.com/api/trans/vip/translate'
            params = {
                'q': query,
                'from': from_lang,
                'to': to_lang,
                'appid': appid,
                'salt': salt,
                'sign': sign
            }
            
            response = requests.get(url, params=params, timeout=10)
            result = response.json()
            
            if 'trans_result' in result:
                trans_result = result['trans_result'][0]['dst']
                parts = trans_result.split('\n')
                translated_title = parts[0].strip()
                translated_summary = parts[1].strip() if len(parts) > 1 else ''
                return {
                    'title': translated_title,
                    'summary': translated_summary
                }
            else:
                print(f"âš ï¸ ç™¾åº¦ç¿»è¯‘å¤±è´¥: {result.get('error_msg', 'æœªçŸ¥é”™è¯¯')}")
                # ç¿»è¯‘å¤±è´¥ä¹Ÿè¿”å›æ ‡è®°
                return {
                    'title': f"{title} (æœªç¿»è¯‘)",
                    'summary': f"{summary} (æœªç¿»è¯‘)" if summary else "(æœªç¿»è¯‘)"
                }
        except Exception as e:
            print(f"âš ï¸ ç™¾åº¦ç¿»è¯‘è¯·æ±‚å¤±è´¥: {e}")
            # å¼‚å¸¸æƒ…å†µä¸‹ä¹Ÿè¿”å›æ ‡è®°
            return {
                'title': f"{title} (æœªç¿»è¯‘)",
                'summary': f"{summary} (æœªç¿»è¯‘)" if summary else "(æœªç¿»è¯‘)"
            }
    
    # ==================== åŸæœ‰AIæ–°é—»æŠ“å–æ–¹æ³•ï¼ˆä¿æŒä¸å˜ï¼‰ ====================
    def fetch_arxiv(self, source):
        """æŠ“å–Arxiv AIè®ºæ–‡"""
        try:
            response = requests.get(source['url'], headers=self.headers, timeout=15)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                dt_list = soup.find_all('dt')
                dd_list = soup.find_all('dd')
                
                for i, (dt, dd) in enumerate(zip(dt_list[:8], dd_list[:8])):
                    paper_id_elem = dt.find('a', title='Abstract')
                    if not paper_id_elem:
                        continue
                    
                    paper_id = paper_id_elem.text.strip()
                    title_elem = dd.find('div', class_='list-title')
                    authors_elem = dd.find('div', class_='list-authors')
                    abstract_elem = dd.find('p')
                    
                    if title_elem:
                        title = title_elem.text.replace('Title:', '').strip()
                        authors = authors_elem.text.replace('Authors:', '').strip() if authors_elem else ''
                        abstract = abstract_elem.text.strip() if abstract_elem else ''
                        
                        article = {
                            'id': f"arxiv_{paper_id}",
                            'title': f"[è®ºæ–‡] {title[:120]}",
                            'link': f'https://arxiv.org/abs/{paper_id}',
                            'source': source['name'],
                            'summary': abstract[:200] + '...' if len(abstract) > 200 else abstract,
                            'authors': authors,
                            'category': 'research',
                            'importance': 9,
                            'time': datetime.now().strftime('%Y-%m-%d'),
                            'type': 'ai',
                            'lang': 'en'  # æ·»åŠ  lang ä»¥æ”¯æŒç¿»è¯‘
                        }
                        # æ·»åŠ ç¿»è¯‘
                        translated = self.baidu_translate(article['title'], article['summary'])
                        # translated ç°åœ¨æ€»æ˜¯è¿”å›å­—å…¸ï¼ŒåŒ…å«åŸå§‹å†…å®¹+æ ‡è®°
                        article['title_translated'] = translated['title']
                        article['summary_translated'] = translated['summary']
                        self.all_articles.append(article)
                        self.ai_articles.append(article)
        except Exception as e:
            print(f"âš ï¸ ArxivæŠ“å–å¤±è´¥: {e}")
    
    async def fetch_rss_async(self, session, source, article_type='ai'):
        """å¼‚æ­¥RSSæŠ“å–æ–¹æ³•"""
        if not ASYNC_AVAILABLE:
            raise RuntimeError("å¼‚æ­¥åŠŸèƒ½ä¸å¯ç”¨ï¼šè¯·å…ˆå®‰è£… aiohttp (pip install aiohttp)")
        
        try:
            async with session.get(source['url'], timeout=aiohttp.ClientTimeout(total=15)) as response:
                if response.status == 200:
                    text = await response.text()
                    feed = feedparser.parse(text)
                    articles_added = 0
                    seen_links = set()
                    
                    for entry in feed.entries[:20]:
                        if articles_added >= 5:
                            break
                        
                        # æ£€æŸ¥å‘å¸ƒæ—¶é—´
                        pub_time = None
                        if hasattr(entry, 'published_parsed'):
                            pub_time = datetime(*entry.published_parsed[:6])
                        elif hasattr(entry, 'updated_parsed'):
                            pub_time = datetime(*entry.updated_parsed[:6])
                        
                        if not pub_time:
                            pub_time = datetime.now()
                            article_importance = 4
                        
                        if pub_time < self.forty_eight_hours_ago:
                            continue
                        
                        title = entry.get('title', '').strip()
                        summary = entry.get('summary', '').strip()
                        link = entry.get('link', '').strip()
                        
                        link_hash = hashlib.md5(link.encode()).hexdigest()
                        if link_hash in seen_links:
                            continue
                        seen_links.add(link_hash)
                        
                        if summary:
                            soup = BeautifulSoup(summary, 'html.parser')
                            summary = soup.get_text()[:250]
                        
                        article = {
                            'id': link_hash[:8],
                            'title': title[:150],
                            'link': link,
                            'source': source['name'],
                            'summary': summary[:250] + '...' if len(summary) > 250 else summary,
                            'category': source.get('category', 'general'),
                            'lang': source.get('lang', 'en'),
                            'importance': 6,
                            'time': pub_time.strftime('%Y-%m-%d %H:%M'),
                            'type': article_type
                        }
                        
                        if article['lang'] == 'en':
                            translated = self.baidu_translate(title, summary)
                            article['title_translated'] = translated['title']
                            article['summary_translated'] = translated['summary']
                        
                        if article_type == 'ai':
                            content = f"{title} {summary}".lower()
                            ai_keywords = ['ai', 'artificial intelligence', 'machine learning', 
                                      'deep learning', 'neural network', 'llm', 'gpt', 'transformer',
                                      'äººå·¥æ™ºèƒ½', 'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'å¤§æ¨¡å‹', 'ç”Ÿæˆå¼AI', 'è®¡ç®—æœºè§†è§‰', 'å›¾åƒç”Ÿæˆ','è®­ç»ƒ',
                                      'AIGC', 'Diffusionæ¨¡å‹', 'MoEæ¨¡å‹', 'RLHF']
                            
                            is_ai_related = any(keyword in content for keyword in ai_keywords)
                            if is_ai_related:
                                article['importance'] = 8
                                self.all_articles.append(article)
                                self.ai_articles.append(article)
                                articles_added += 1
                        else:
                            if link_hash not in [a['id'] for a in self.fact_articles]:
                                self.all_articles.append(article)
                                self.fact_articles.append(article)
                                articles_added += 1
                    
                    print(f"  âœ“ {source['name']} æŠ“å–å®Œæˆ ({articles_added}ç¯‡)")
                    return articles_added
                else:
                    print(f"  âŒ {source['name']} HTTP {response.status}")
                    return 0
        except Exception as e:
            print(f"  âŒ {source['name']} æŠ“å–å‡ºé”™: {e}")
            return 0
    
    async def fetch_hackernews_async(self, session, source, article_type='ai'):
        """å¼‚æ­¥Hacker NewsæŠ“å–æ–¹æ³•"""
        if not ASYNC_AVAILABLE:
            raise RuntimeError("å¼‚æ­¥åŠŸèƒ½ä¸å¯ç”¨ï¼šè¯·å…ˆå®‰è£… aiohttp (pip install aiohttp)")
        
        try:
            timestamp = int(self.forty_eight_hours_ago.timestamp())
            query_param = source['url'].format(timestamp)
            url = query_param
            
            if article_type == 'fact' and 'query=AI' in url:
                url = url.replace('&query=AI', '')
            
            async with session.get(url, timeout=aiohttp.ClientTimeout(total=10)) as response:
                if response.status == 200:
                    data = await response.json()
                    hits = data.get('hits', [])
                    seen_links = set()
                    count = 0
                    
                    for hit in hits[:10]:
                        link = hit.get('url', f"https://news.ycombinator.com/item?id={hit.get('objectID')}")
                        link_hash = hashlib.md5(link.encode()).hexdigest()
                        if link_hash in seen_links:
                            continue
                        seen_links.add(link_hash)
                        
                        title = hit.get('title', '')
                        
                        if article_type == 'ai' and not any(keyword in title.lower() for keyword in ['ai', 'llm', 'gpt', 'openai', 'anthropic']):
                            continue
                        
                        article = {
                            'id': f"hn_{hit.get('objectID', '')}",
                            'title': title,
                            'link': link,
                            'source': source['name'],
                            'points': hit.get('points', 0),
                            'comments': hit.get('num_comments', 0),
                            'category': source.get('category', 'tech'),
                            'importance': min(9, 6 + (hit.get('points', 0) // 20)),
                            'time': datetime.fromtimestamp(hit.get('created_at_i', 0)).strftime('%Y-%m-%d %H:%M'),
                            'type': article_type,
                            'lang': source.get('lang', 'en')
                        }
                        
                        if article['lang'] == 'en':
                            translated = self.baidu_translate(title, '')
                            article['title_translated'] = translated['title']
                        
                        self.all_articles.append(article)
                        if article_type == 'ai':
                            self.ai_articles.append(article)
                        else:
                            self.fact_articles.append(article)
                        count += 1
                    
                    print(f"  âœ“ {source['name']} æŠ“å–å®Œæˆ ({count}ç¯‡)")
                    return count
                else:
                    print(f"  âŒ {source['name']} HTTP {response.status}")
                    return 0
        except Exception as e:
            print(f"  âŒ {source['name']} æŠ“å–å‡ºé”™: {e}")
            return 0
        """é€šç”¨RSSæŠ“å–æ–¹æ³•ï¼Œå¢å¼ºå»é‡å’Œæ—¶æ•ˆæ€§"""
        try:
            feed = feedparser.parse(source['url'])
            articles_added = 0
            seen_links = set()  # å¢å¼ºå»é‡
            
            for entry in feed.entries[:20]:  # å¢åŠ æ£€æŸ¥èŒƒå›´ä»¥è·å–æ›´å¤šæ–°é²œå†…å®¹
                if articles_added >= 5:  # æ¯ä¸ªæºæœ€å¤šå–5æ¡
                    break
                    
                # æ£€æŸ¥å‘å¸ƒæ—¶é—´
                pub_time = None
                if hasattr(entry, 'published_parsed'):
                    pub_time = datetime(*entry.published_parsed[:6])
                elif hasattr(entry, 'updated_parsed'):
                    pub_time = datetime(*entry.updated_parsed[:6])
                
                # å¦‚æœæ— æ³•è·å–æ—¶é—´ï¼Œä½¿ç”¨å½“å‰æ—¶é—´ä½†é™ä½ä¼˜å…ˆçº§
                if not pub_time:
                    pub_time = datetime.now()
                    article_importance = 4  # é™ä½æœªçŸ¥æ—¶é—´çš„é‡è¦æ€§
                
                # æ£€æŸ¥æ˜¯å¦åœ¨è¿‡å»48å°æ—¶å†…
                if pub_time < self.forty_eight_hours_ago:
                    continue
                
                title = entry.get('title', '').strip()
                summary = entry.get('summary', '').strip()
                link = entry.get('link', '').strip()
                
                # å»é‡æ£€æŸ¥
                link_hash = hashlib.md5(link.encode()).hexdigest()
                if link_hash in seen_links:
                    continue
                seen_links.add(link_hash)
                
                # æ¸…ç†HTMLæ ‡ç­¾
                if summary:
                    soup = BeautifulSoup(summary, 'html.parser')
                    summary = soup.get_text()[:250]
                
                article = {
                    'id': link_hash[:8],
                    'title': title[:150],
                    'link': link,
                    'source': source['name'],
                    'summary': summary[:250] + '...' if len(summary) > 250 else summary,
                    'category': source.get('category', 'general'),
                    'lang': source.get('lang', 'en'),
                    'importance': 6,
                    'time': pub_time.strftime('%Y-%m-%d %H:%M'),
                    'type': article_type
                }
                
                # å¦‚æœæ˜¯è‹±æ–‡ï¼Œè¿›è¡Œç¿»è¯‘ä»¥æä¾›ä¸­è‹±æ–‡å¯¹ç…§
                if article['lang'] == 'en':
                    translated = self.baidu_translate(title, summary)
                    # translated ç°åœ¨æ€»æ˜¯è¿”å›å­—å…¸ï¼ŒåŒ…å«åŸå§‹å†…å®¹+æ ‡è®°
                    article['title_translated'] = translated['title']
                    article['summary_translated'] = translated['summary']
                
                # å¦‚æœæ˜¯AIæ–°é—»æºï¼Œæ£€æŸ¥æ˜¯å¦AIç›¸å…³
                if article_type == 'ai':
                    content = f"{title} {summary}".lower()
                    ai_keywords = ['ai', 'artificial intelligence', 'machine learning', 
                              'deep learning', 'neural network', 'llm', 'gpt', 'transformer',
                              'äººå·¥æ™ºèƒ½', 'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'å¤§æ¨¡å‹', 'ç”Ÿæˆå¼AI', 'è®¡ç®—æœºè§†è§‰', 'å›¾åƒç”Ÿæˆ','è®­ç»ƒ',
                              'AIGC', 'Diffusionæ¨¡å‹', 'MoEæ¨¡å‹', 'RLHF']
                    
                    is_ai_related = any(keyword in content for keyword in ai_keywords)
                    if is_ai_related:
                        article['importance'] = 8
                        self.all_articles.append(article)
                        self.ai_articles.append(article)
                        articles_added += 1
                else:
                    # äº‹å®æ–°é—»ç›´æ¥æ·»åŠ ï¼Œæ£€æŸ¥é‡å¤
                    if link_hash not in [a['id'] for a in self.fact_articles]:
                        self.all_articles.append(article)
                        self.fact_articles.append(article)
                        articles_added += 1
                    
        except Exception as e:
            print(f"âš ï¸ RSSæŠ“å–å¤±è´¥ {source['name']}: {e}")
    
    def fetch_hackernews(self, source, article_type='ai'):
        """é€šç”¨Hacker NewsæŠ“å–æ–¹æ³•"""
        try:
            timestamp = int(self.forty_eight_hours_ago.timestamp())
            query_param = source['url'].format(timestamp)
            url = query_param
            
            # å¦‚æœä¸æ˜¯AIä¸“ç”¨æœç´¢ï¼Œç§»é™¤AIæŸ¥è¯¢å‚æ•°
            if article_type == 'fact' and 'query=AI' in url:
                url = url.replace('&query=AI', '')
            
            response = requests.get(url, timeout=10)
            
            if response.status_code == 200:
                hits = response.json().get('hits', [])
                seen_links = set()
                for hit in hits[:10]:
                    link = hit.get('url', f"https://news.ycombinator.com/item?id={hit.get('objectID')}")
                    link_hash = hashlib.md5(link.encode()).hexdigest()
                    if link_hash in seen_links:
                        continue
                    seen_links.add(link_hash)
                    
                    title = hit.get('title', '')
                    
                    # å¯¹äºäº‹å®æ–°é—»ï¼Œä¸ç­›é€‰AIå†…å®¹
                    if article_type == 'ai' and not any(keyword in title.lower() for keyword in ['ai', 'llm', 'gpt', 'openai', 'anthropic']):
                        continue
                    
                    article = {
                        'id': f"hn_{hit.get('objectID', '')}",
                        'title': title,
                        'link': link,
                        'source': source['name'],
                        'points': hit.get('points', 0),
                        'comments': hit.get('num_comments', 0),
                        'category': source.get('category', 'tech'),
                        'importance': min(9, 6 + (hit.get('points', 0) // 20)),
                        'time': datetime.fromtimestamp(hit.get('created_at_i', 0)).strftime('%Y-%m-%d %H:%M'),
                        'type': article_type,
                        'lang': source.get('lang', 'en')
                    }
                    
                    # ç¿»è¯‘å¦‚æœè‹±æ–‡
                    if article['lang'] == 'en':
                        translated = self.baidu_translate(title, '')
                        # translated ç°åœ¨æ€»æ˜¯è¿”å›å­—å…¸ï¼ŒåŒ…å«åŸå§‹å†…å®¹+æ ‡è®°
                        article['title_translated'] = translated['title']
                    
                    self.all_articles.append(article)
                    if article_type == 'ai':
                        self.ai_articles.append(article)
                    else:
                        self.fact_articles.append(article)
                        
        except Exception as e:
            print(f"âš ï¸ Hacker NewsæŠ“å–å¤±è´¥: {e}")
    
    # ==================== æ–°å¢ï¼šæŠ“å–äº‹å®æ–°é—» ====================
    def fetch_fact_news(self):
        """æŠ“å–å¤šæ–¹é¢äº‹å®æ–°é—»"""
        print("\nğŸ“° å¼€å§‹æŠ“å–å¤šæ–¹é¢äº‹å®æ–°é—»ï¼ˆè¿‡å»48å°æ—¶ï¼‰...")
        
        for source in self.fact_news_sources:
            print(f"  â†’ {source['name']}")
            try:
                if source['type'] == 'rss':
                    self.fetch_rss(source, article_type='fact')
                elif source['type'] == 'hn_api':
                    self.fetch_hackernews(source, article_type='fact')
                time.sleep(1)  # ç¤¼è²Œå»¶è¿Ÿ
            except Exception as e:
                print(f"    âŒ æŠ“å–å¤±è´¥: {e}")
                continue
        
        print(f"âœ… äº‹å®æ–°é—»æŠ“å–å®Œæˆï¼å…±è·å¾— {len(self.fact_articles)} ç¯‡")
        
        # å»é‡å’Œç­›é€‰æœ€é‡è¦çš„10ç¯‡
        unique_facts = []
        seen_ids = set()
        for article in self.fact_articles:
            if article['id'] not in seen_ids:
                unique_facts.append(article)
                seen_ids.add(article['id'])
        self.fact_articles = sorted(
            unique_facts, 
            key=lambda x: (x.get('importance', 5), datetime.strptime(x['time'], '%Y-%m-%d %H:%M') if x.get('time') else datetime.now()), 
            reverse=True
        )[:10]
    
    # ==================== åŸæœ‰AIåˆ†æåŠŸèƒ½ï¼ˆä¿æŒä¸å˜ï¼‰ ====================
    def fetch_rss(self, source, article_type='ai'):
        """é€šç”¨RSSæŠ“å–æ–¹æ³•ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼Œå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
        max_retries = 3
        retry_delay = 2  # ç§’
        
        for attempt in range(max_retries):
            try:
                import requests
                # æ·»åŠ æ›´çœŸå®çš„è¯·æ±‚å¤´
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                    'Accept-Encoding': 'gzip, deflate, br',
                    'Connection': 'keep-alive',
                    'Upgrade-Insecure-Requests': '1',
                }
                
                response = requests.get(source['url'], headers=headers, timeout=20)
                
                # æ£€æŸ¥å“åº”çŠ¶æ€
                if response.status_code == 404:
                    print(f"  âš ï¸  {source['name']} é¡µé¢ä¸å­˜åœ¨ (404)")
                    if attempt < max_retries - 1:
                        print(f"     å°è¯•ç¬¬ {attempt + 2} æ¬¡...")
                        time.sleep(retry_delay)
                        continue
                    return 0
                elif response.status_code == 403:
                    print(f"  âš ï¸  {source['name']} è®¿é—®è¢«æ‹’ç» (403)ï¼Œå¯èƒ½éœ€è¦ä»£ç†æˆ–é™ä½é¢‘ç‡")
                    if attempt < max_retries - 1:
                        print(f"     å°è¯•ç¬¬ {attempt + 2} æ¬¡...")
                        time.sleep(retry_delay * 2)  # 403é”™è¯¯ç­‰å¾…æ›´é•¿æ—¶é—´
                        continue
                    return 0
                elif response.status_code != 200:
                    print(f"  âš ï¸  {source['name']} HTTP {response.status_code}")
                    if attempt < max_retries - 1:
                        print(f"     å°è¯•ç¬¬ {attempt + 2} æ¬¡...")
                        time.sleep(retry_delay)
                        continue
                    return 0
                
                # æˆåŠŸè·å–å†…å®¹
                feed = feedparser.parse(response.text)
                articles_added = 0
                seen_links = set()
                
                for entry in feed.entries[:20]:
                    if articles_added >= 5:
                        break
                        
                    # æ£€æŸ¥å‘å¸ƒæ—¶é—´
                    pub_time = None
                    if hasattr(entry, 'published_parsed'):
                        pub_time = datetime(*entry.published_parsed[:6])
                    elif hasattr(entry, 'updated_parsed'):
                        pub_time = datetime(*entry.updated_parsed[:6])
                    
                    if not pub_time:
                        pub_time = datetime.now()
                        article_importance = 4
                    
                    if pub_time < self.forty_eight_hours_ago:
                        continue
                    
                    title = entry.get('title', '').strip()
                    summary = entry.get('summary', '').strip()
                    link = entry.get('link', '').strip()
                    
                    # è·³è¿‡ç©ºæ ‡é¢˜æˆ–é“¾æ¥
                    if not title or not link:
                        continue
                        
                    link_hash = hashlib.md5(link.encode()).hexdigest()
                    if link_hash in seen_links:
                        continue
                    seen_links.add(link_hash)
                    
                    if summary:
                        soup = BeautifulSoup(summary, 'html.parser')
                        summary = soup.get_text()[:250]
                    
                    article = {
                        'id': link_hash[:8],
                        'title': title[:150],
                        'link': link,
                        'source': source['name'],
                        'summary': summary[:250] + '...' if len(summary) > 250 else summary,
                        'category': source.get('category', 'general'),
                        'lang': source.get('lang', 'en'),
                        'importance': 6,
                        'time': pub_time.strftime('%Y-%m-%d %H:%M'),
                        'type': article_type
                    }
                    
                    if article['lang'] == 'en':
                        translated = self.baidu_translate(title, summary)
                        article['title_translated'] = translated['title']
                        article['summary_translated'] = translated['summary']
                    
                    if article_type == 'ai':
                        content = f"{title} {summary}".lower()
                        ai_keywords = ['ai', 'artificial intelligence', 'machine learning', 
                                  'deep learning', 'neural network', 'llm', 'gpt', 'transformer',
                                  'äººå·¥æ™ºèƒ½', 'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'å¤§æ¨¡å‹', 'ç”Ÿæˆå¼AI', 'è®¡ç®—æœºè§†è§‰', 'å›¾åƒç”Ÿæˆ','è®­ç»ƒ',
                                  'AIGC', 'Diffusionæ¨¡å‹', 'MoEæ¨¡å‹', 'RLHF']
                        
                        is_ai_related = any(keyword in content for keyword in ai_keywords)
                        if is_ai_related:
                            article['importance'] = 8
                            self.all_articles.append(article)
                            self.ai_articles.append(article)
                            articles_added += 1
                    else:
                        if link_hash not in [a['id'] for a in self.fact_articles]:
                            self.all_articles.append(article)
                            self.fact_articles.append(article)
                            articles_added += 1
                
                if articles_added > 0:
                    print(f"  âœ“ {source['name']} æŠ“å–å®Œæˆ ({articles_added}ç¯‡)")
                else:
                    print(f"  âš ï¸  {source['name']} æ— æ–°å†…å®¹")
                return articles_added
                
            except requests.exceptions.Timeout:
                print(f"  âš ï¸  {source['name']} è¯·æ±‚è¶…æ—¶")
                if attempt < max_retries - 1:
                    print(f"     å°è¯•ç¬¬ {attempt + 2} æ¬¡...")
                    time.sleep(retry_delay)
                    continue
                return 0
            except requests.exceptions.ConnectionError:
                print(f"  âš ï¸  {source['name']} è¿æ¥é”™è¯¯")
                if attempt < max_retries - 1:
                    print(f"     å°è¯•ç¬¬ {attempt + 2} æ¬¡...")
                    time.sleep(retry_delay)
                    continue
                return 0
            except Exception as e:
                print(f"  âš ï¸  {source['name']} æŠ“å–å‡ºé”™: {e}")
                if attempt < max_retries - 1:
                    print(f"     å°è¯•ç¬¬ {attempt + 2} æ¬¡...")
                    time.sleep(retry_delay)
                    continue
                return 0
        
        return 0  # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
    
    def fetch_hackernews(self, source, article_type='ai'):
        """é€šç”¨Hacker NewsæŠ“å–æ–¹æ³•ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼Œå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
        max_retries = 3
        retry_delay = 2
        
        for attempt in range(max_retries):
            try:
                import requests
                timestamp = int(self.forty_eight_hours_ago.timestamp())
                query_param = source['url'].format(timestamp)
                url = query_param
                
                if article_type == 'fact' and 'query=AI' in url:
                    url = url.replace('&query=AI', '')
                
                # ä½¿ç”¨æ›´çœŸå®çš„è¯·æ±‚å¤´
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                    'Accept': 'application/json, text/plain, */*',
                    'Accept-Language': 'en-US,en;q=0.9',
                }
                
                response = requests.get(url, headers=headers, timeout=15)
                
                if response.status_code != 200:
                    print(f"  âš ï¸  {source['name']} HTTP {response.status_code}")
                    if attempt < max_retries - 1:
                        print(f"     å°è¯•ç¬¬ {attempt + 2} æ¬¡...")
                        time.sleep(retry_delay)
                        continue
                    return 0
                
                hits = response.json().get('hits', [])
                seen_links = set()
                count = 0
                
                for hit in hits[:10]:
                    link = hit.get('url', f"https://news.ycombinator.com/item?id={hit.get('objectID')}")
                    link_hash = hashlib.md5(link.encode()).hexdigest()
                    if link_hash in seen_links:
                        continue
                    seen_links.add(link_hash)
                    
                    title = hit.get('title', '')
                    
                    if article_type == 'ai' and not any(keyword in title.lower() for keyword in ['ai', 'llm', 'gpt', 'openai', 'anthropic']):
                        continue
                    
                    article = {
                        'id': f"hn_{hit.get('objectID', '')}",
                        'title': title,
                        'link': link,
                        'source': source['name'],
                        'points': hit.get('points', 0),
                        'comments': hit.get('num_comments', 0),
                        'category': source.get('category', 'tech'),
                        'importance': min(9, 6 + (hit.get('points', 0) // 20)),
                        'time': datetime.fromtimestamp(hit.get('created_at_i', 0)).strftime('%Y-%m-%d %H:%M'),
                        'type': article_type,
                        'lang': source.get('lang', 'en')
                    }
                    
                    if article['lang'] == 'en':
                        translated = self.baidu_translate(title, '')
                        article['title_translated'] = translated['title']
                    
                    self.all_articles.append(article)
                    if article_type == 'ai':
                        self.ai_articles.append(article)
                    else:
                        self.fact_articles.append(article)
                    count += 1
                
                if count > 0:
                    print(f"  âœ“ {source['name']} æŠ“å–å®Œæˆ ({count}ç¯‡)")
                else:
                    print(f"  âš ï¸  {source['name']} æ— ç¬¦åˆæ¡ä»¶çš„å†…å®¹")
                return count
                
            except requests.exceptions.Timeout:
                print(f"  âš ï¸  {source['name']} è¯·æ±‚è¶…æ—¶")
                if attempt < max_retries - 1:
                    print(f"     å°è¯•ç¬¬ {attempt + 2} æ¬¡...")
                    time.sleep(retry_delay)
                    continue
                return 0
            except requests.exceptions.ConnectionError:
                print(f"  âš ï¸  {source['name']} è¿æ¥é”™è¯¯")
                if attempt < max_retries - 1:
                    print(f"     å°è¯•ç¬¬ {attempt + 2} æ¬¡...")
                    time.sleep(retry_delay)
                    continue
                return 0
            except Exception as e:
                print(f"  âš ï¸  {source['name']} æŠ“å–å‡ºé”™: {e}")
                if attempt < max_retries - 1:
                    print(f"     å°è¯•ç¬¬ {attempt + 2} æ¬¡...")
                    time.sleep(retry_delay)
                    continue
                return 0
        
        return 0
    
    def fetch_all_news(self):
        """æŠ“å–æ‰€æœ‰æ–°é—»"""
        print("ğŸ“¡ å¼€å§‹æŠ“å–AIç§‘æŠ€æ–°é—»ï¼ˆè¿‡å»48å°æ—¶ï¼‰...")
        for source in self.ai_news_sources:
            print(f"  â†’ {source['name']}")
            try:
                if source['type'] == 'arxiv':
                    self.fetch_arxiv(source)
                elif source['type'] == 'rss':
                    self.fetch_rss(source, article_type='ai')
                elif source['type'] == 'hn_api':
                    self.fetch_hackernews(source, article_type='ai')
                time.sleep(1)
            except Exception as e:
                print(f"    âŒ æŠ“å–å¤±è´¥: {e}")
        
        print(f"âœ… AIæ–°é—»æŠ“å–å®Œæˆï¼å…±è·å¾— {len(self.ai_articles)} ç¯‡")
    


    def fetch_arxiv_abstract(self, url):
        """ä» arXiv è®ºæ–‡è¯¦æƒ…é¡µæå–å®Œæ•´æ‘˜è¦ï¼ˆå¸¦ç¼“å­˜æœºåˆ¶ï¼‰"""
        # æ£€æŸ¥ç¼“å­˜ï¼Œé¿å…é‡å¤è¯·æ±‚
        if url in self.abstract_cache:
            print(f"  âœ“ ä½¿ç”¨ç¼“å­˜çš„æ‘˜è¦: {url}")
            return self.abstract_cache[url]
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()  # é 200 æŠ›å¼‚å¸¸
            
            soup = BeautifulSoup(response.text, 'html.parser')
            abstract_tag = soup.find('blockquote', class_='abstract')
            if abstract_tag:
                abstract_text = abstract_tag.text.strip()
                # å­˜å…¥ç¼“å­˜
                self.abstract_cache[url] = abstract_text
                return abstract_text
            else:
                print(f"  âš ï¸  æœªæ‰¾åˆ°æ‘˜è¦æ ‡ç­¾: {url}")
                self.abstract_cache[url] = ""
                return ""
        except Exception as e:
            print(f"  âŒ æŠ“å– arXiv æ‘˜è¦å¤±è´¥ {url}: {e}")
            self.abstract_cache[url] = ""
            return ""

    def analyze_with_gemini(self, article):
        api_key = os.getenv('GEMINI_API_KEY')
        if not api_key:
            print("  âš ï¸  æœªé…ç½® GEMINI_API_KEYï¼Œä½¿ç”¨å¤‡ç”¨åˆ†æ")
            return self._fallback_analysis(article)
    
        try:
            genai.configure(api_key=api_key)
            # ä½¿ç”¨å½“å‰å¯ç”¨çš„ç¨³å®šæ¨¡å‹ï¼ˆ2026å¹´2æœˆæ¨èï¼‰
            # å¯é€‰æ¨¡å‹: 'gemini-1.5-pro', 'gemini-1.0-pro', 'models/gemini-1.0-pro-001'
            model = genai.GenerativeModel('gemini-1.5-pro')
    
            # å¦‚æœæ˜¯ArXivï¼Œä¼˜å…ˆè·å–çœŸå®æ‘˜è¦ï¼ˆå·²å¸¦ç¼“å­˜ï¼‰
            full_abstract = ""
            if 'arxiv.org' in article['link']:
                full_abstract = self.fetch_arxiv_abstract(article['link'])
    
            prompt = f"""ä½œä¸ºä¸“ä¸šAIè®ºæ–‡åˆ†æå¸ˆï¼Œè¯·åˆ†æä»¥ä¸‹è®ºæ–‡ï¼š
    
    æ ‡é¢˜ï¼š{article['title']}
    æ¥æºï¼š{article['source']}
    æ‘˜è¦ï¼š{full_abstract or article.get('summary', 'æš‚æ— æ‘˜è¦')}
    
    è¯·ä¸¥æ ¼è¾“å‡ºJSONï¼š
    {{
      "content_summary": "150-250å­—ä¸­æ–‡æ‘˜è¦ï¼Œæç‚¼æ ¸å¿ƒè´¡çŒ®ã€æ–¹æ³•ã€ç»“æœ",
      "content_tags": ["æ ‡ç­¾1", "æ ‡ç­¾2", ...],
      "importance_level": "é«˜/ä¸­/ä½",
      "impact_scope": "å¤šæ–¹é¢å½±å“æè¿°",
      "attention_reason": "ç»“åˆåˆ›æ–°ã€ç»“æœã€å±€é™çš„å¤šè§’åº¦ç†ç”±",
      "key_points": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"]
    }}
    
    ç›´æ¥è¿”å›JSONï¼Œæ— å¤šä½™æ–‡å­—ã€‚
    """
    
            response = model.generate_content(prompt)
            text = response.text.strip()
            json_match = re.search(r'\{.*\}', text, re.DOTALL)
            if json_match:
                analysis_result = json.loads(json_match.group(0))
                # ç¡®ä¿ key_points å­˜åœ¨
                if 'key_points' not in analysis_result:
                    analysis_result['key_points'] = analysis_result.get('content_tags', [])[:3]
                return analysis_result
            else:
                print(f"  âš ï¸  Geminiè¿”å›æ ¼å¼å¼‚å¸¸ï¼Œæ— æ³•è§£æJSONï¼Œä½¿ç”¨å¤‡ç”¨åˆ†æ")
                return self._fallback_analysis(article)
        except json.JSONDecodeError as e:
            print(f"  âŒ Geminiåˆ†æJSONè§£æå¤±è´¥:")
            print(f"     é”™è¯¯ç±»å‹: {type(e).__name__}")
            print(f"     é”™è¯¯ä¿¡æ¯: {str(e)}")
            print(f"     æ–‡ç« æ ‡é¢˜: {article.get('title', 'N/A')[:80]}")
            return self._fallback_analysis(article)
        except requests.exceptions.RequestException as e:
            print(f"  âŒ Gemini APIç½‘ç»œè¯·æ±‚å¤±è´¥:")
            print(f"     é”™è¯¯ç±»å‹: {type(e).__name__}")
            print(f"     é”™è¯¯ä¿¡æ¯: {str(e)}")
            print(f"     æ–‡ç« æ ‡é¢˜: {article.get('title', 'N/A')[:80]}")
            return self._fallback_analysis(article)
        except Exception as e:
            error_msg = str(e)
            if "not found for API version" in error_msg or "404" in error_msg:
                print(f"  âŒ Geminiæ¨¡å‹ä¸å¯ç”¨é”™è¯¯:")
                print(f"     é”™è¯¯ä¿¡æ¯: {error_msg}")
                print(f"     å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
                print(f"     1. å°è¯•ä½¿ç”¨ 'gemini-1.5-pro' æˆ– 'gemini-1.0-pro'")
                print(f"     2. æ£€æŸ¥Google AI Studioä¸­çš„å¯ç”¨æ¨¡å‹")
                print(f"     3. æ›´æ–°google-generativeaiåº“: pip install --upgrade google-generativeai")
            else:
                print(f"  âŒ Geminiåˆ†æå‘ç”Ÿæœªé¢„æœŸé”™è¯¯:")
                print(f"     é”™è¯¯ç±»å‹: {type(e).__name__}")
                print(f"     é”™è¯¯ä¿¡æ¯: {str(e)}")
                print(f"     æ–‡ç« é“¾æ¥: {article.get('link', 'N/A')}")
                print(f"     æ–‡ç« æ ‡é¢˜: {article.get('title', 'N/A')[:80]}")
                import traceback
                print(f"     å †æ ˆè·Ÿè¸ª:\n{traceback.format_exc()}")
            return self._fallback_analysis(article)
        
    def _fallback_analysis(self, article):
        """å¤‡ç”¨å…³é”®è¯åˆ†æï¼Œå½“ Gemini å¤±è´¥æ—¶ä½¿ç”¨"""
        text = f"{article['title']} {article.get('summary', '')}".lower()
        
        tags = []
        
        # æ”¿æ²»ç›¸å…³
        if any(word in text for word in ['politics', 'government', 'policy', 'æ”¿æ²»', 'æ”¿åºœ', 'æ”¿ç­–']):
            tags.append('æ”¿æ²»')
        
        # ç»æµç›¸å…³
        if any(word in text for word in ['economy', 'financial', 'market', 'ç»æµ', 'é‡‘è', 'å¸‚åœº']):
            tags.append('ç»æµ')
        
        # ç§‘æŠ€ç›¸å…³
        if any(word in text for word in ['technology', 'tech', 'digital', 'ç§‘æŠ€', 'æŠ€æœ¯', 'æ•°å­—åŒ–']):
            tags.append('ç§‘æŠ€')
        
        # å¥åº·åŒ»ç–—
        if any(word in text for word in ['health', 'medical', 'ç–«æƒ…', 'ç–«è‹—', 'å¥åº·', 'åŒ»ç–—']):
            tags.append('å¥åº·')
        
        # ç¯å¢ƒç”Ÿæ€
        if any(word in text for word in ['environment', 'climate', 'ç¯ä¿', 'æ°”å€™', 'ç¯å¢ƒ', 'ç”Ÿæ€']):
            tags.append('ç¯å¢ƒ')
        
        # AI ç‰¹å®šå…³é”®è¯ï¼ˆæ‰©å±•ï¼‰
        ai_keywords = [
            'ai', 'llm', 'gpt', 'transformer', 'äººå·¥æ™ºèƒ½', 'å¤§æ¨¡å‹', 'ç”Ÿæˆå¼ai',
            'reasoning', 'æ¨ç†', 'chain of thought', 'æ€ç»´é“¾', 'cot',
            'routing', 'è·¯ç”±', 'router', 'åˆ†å‘',
            'agent', 'æ™ºèƒ½ä½“', 'è‡ªä¸»ä»£ç†',
            'rlhf', 'äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ', 'reinforcement learning',
            'fine-tuning', 'å¾®è°ƒ', 'adapter', 'é€‚é…å™¨',
            'multimodal', 'å¤šæ¨¡æ€', 'vision', 'å›¾åƒ', 'audio', 'éŸ³é¢‘',
            'embedding', 'åµŒå…¥', 'vector', 'å‘é‡',
            'attention', 'æ³¨æ„åŠ›', 'self-attention', 'è‡ªæ³¨æ„åŠ›',
            'few-shot', 'å°‘æ ·æœ¬', 'zero-shot', 'é›¶æ ·æœ¬',
            'prompt', 'æç¤ºè¯', 'instruction', 'æŒ‡ä»¤',
            'benchmark', 'åŸºå‡†æµ‹è¯•', 'evaluation', 'è¯„ä¼°',
            'moe', 'æ··åˆä¸“å®¶', 'sparse', 'ç¨€ç–',
            'diffusion', 'æ‰©æ•£æ¨¡å‹', 'stable diffusion',
            'rl', 'å¼ºåŒ–å­¦ä¹ ', 'q-learning',
            'nlp', 'è‡ªç„¶è¯­è¨€å¤„ç†', 'computer vision', 'è®¡ç®—æœºè§†è§‰',
            'robotics', 'æœºå™¨äºº', 'autonomous', 'è‡ªåŠ¨é©¾é©¶',
            'ethics', 'ä¼¦ç†', 'bias', 'åè§', 'fairness', 'å…¬å¹³æ€§'
        ]
        
        if any(word in text for word in ai_keywords):
            tags.append('AIç›¸å…³')
        
        # é»˜è®¤æ ‡ç­¾
        if not tags:
            tags = ['ç»¼åˆæ–°é—»']
        
        return {
            'content_summary': "æš‚æ— è¯¦ç»†æ‘˜è¦",
            'content_tags': tags,
            'importance_level': 'ä¸­',
            'impact_scope': 'å¹¿æ³›å…³æ³¨',
            'attention_reason': 'å€¼å¾—å…³æ³¨çš„æ–°é—»æŠ¥é“',
            'key_points': tags
        }
    
    def generate_deep_analyses(self, limit=3):
        """ç”Ÿæˆæ·±åº¦åˆ†æï¼ˆAIæ–°é—»ï¼‰"""
        if not self.ai_articles:
            return []
        
        important_articles = sorted(
            self.ai_articles,
            key=lambda x: x.get('importance', 5),
            reverse=True
        )[:limit]
        
        print(f"\nğŸ” å¼€å§‹æ·±åº¦åˆ†æ {len(important_articles)} ç¯‡AIæ–‡ç« ...")
        
        analyses = []
        for i, article in enumerate(important_articles, 1):
            print(f"  {i}. åˆ†æ: {article['title'][:60]}...")
            analysis = self.analyze_with_gemini(article)
            
            # å¦‚æœæœ‰ç¿»è¯‘ï¼Œä½¿ç”¨ç¿»è¯‘
            title_display = article.get('title_translated', article['title'])
            
            analysis_text = f"""## ğŸ“Š {title_display}

**æ¥æº**: {article['source']} | **æ—¶é—´**: {article.get('time', 'N/A')}
**AIåˆ†ææ¨¡å‹**: ğŸ¤– Gemini

**ğŸ”— åŸæ–‡é“¾æ¥**: {article['link']}

**ğŸ“ å†…å®¹æ‘˜è¦**:
{article.get('summary_translated', article.get('summary', 'æš‚æ— è¯¦ç»†æ‘˜è¦'))}

**ğŸ·ï¸ å†…å®¹æ ‡ç­¾**: {', '.join(analysis['content_tags'])}

**âœ¨ é‡è¦æ€§**: {analysis['importance_level'].upper()}

**ğŸ“ˆ å½±å“èŒƒå›´**: {analysis['impact_scope']}

**ğŸ’¡ å…³æ³¨ç†ç”±**: {analysis['attention_reason']}

**ğŸ”¬ æ ¸å¿ƒè¦ç‚¹**:
{chr(10).join(f'- {point}' for point in analysis['key_points'][:3])}

---
"""
            analyses.append({
                'article': article,
                'analysis': analysis,
                'text': analysis_text
            })
            
            if self.gemini_api_key:
                time.sleep(1)  # APIè°ƒç”¨é—´éš”
        
        self.deep_analyses = analyses
        return analyses
    
    def select_featured_articles(self):
        """é€‰æ‹©ç²¾é€‰æ–‡ç« """
        if self.ai_articles:
            scored_ai = sorted(
                [(a.get('importance', 5), a) for a in self.ai_articles],
                reverse=True, key=lambda x: x[0]
            )
            if scored_ai:
                self.featured_article = scored_ai[0][1]
        
        if self.fact_articles:
            # äº‹å®æ–°é—»æŒ‰é‡è¦æ€§å’Œæ—¶æ•ˆæ€§è¯„åˆ†
            for article in self.fact_articles:
                # åŠ åˆ†é¡¹ï¼šé«˜é‡è¦æ€§ã€å¤šè¯„è®º/åˆ†æ•°ã€è¿‘æœŸå‘å¸ƒ
                score = article.get('importance', 5)
                if article.get('points', 0) > 50:
                    score += 1
                if article.get('comments', 0) > 20:
                    score += 1
                article['_score'] = score
            
            scored_facts = sorted(
                self.fact_articles,
                key=lambda x: x.get('_score', 5),
                reverse=True
            )
            if scored_facts:
                self.featured_fact = scored_facts[0]
    
    def format_fact_news_section(self):
        """æ ¼å¼åŒ–äº‹å®æ–°é—»éƒ¨åˆ†ï¼ŒæŒ‰ä¸­æ–‡/å›½é™…åˆ†ç»„å±•ç¤º"""
        if not self.fact_articles:
            return ""

        section = f"""
## ğŸŒ 48å°æ—¶äº‹å®èµ„è®¯é€Ÿè§ˆ ({len(self.fact_articles)}ç¯‡)

**æ–°é—»æ¥æº**: {', '.join(set([a['source'] for a in self.fact_articles[:10]]))}

"""

        # æŒ‰è¯­è¨€/åœ°åŒºåˆ†ç»„ï¼šä¸­æ–‡ vs è‹±æ–‡
        cn_articles = []
        intl_articles = []
        
        for article in self.fact_articles[:10]:  # æœ€å¤š10ç¯‡
            lang = article.get('lang', 'en')
            if lang == 'zh':
                cn_articles.append(article)
            else:
                intl_articles.append(article)
        
        # 1. ä¸­æ–‡æ–°é—»åŒº
        if cn_articles:
            section += f"\n### ğŸ‡¨ğŸ‡³ ä¸­æ–‡æ–°é—»\n\n"
            for i, article in enumerate(cn_articles, 1):
                emoji = "â­ï¸" if article.get('importance', 0) > 7 else "ğŸ“"
                title = article['title']  # ä¸­æ–‡æ–°é—»ç›´æ¥æ˜¾ç¤ºåŸæ ‡é¢˜
                source = article['source']
                
                section += f"{i}. {emoji} **{title}**\n"
                section += f"   ğŸ“ {source}"
                
                # æ·»åŠ äº’åŠ¨æ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
                if article.get('points', 0) > 0:
                    section += f" | ğŸ‘ {article['points']}"
                if article.get('comments', 0) > 0:
                    section += f" | ğŸ’¬ {article['comments']}"
                
                section += f"\n   ğŸ”— [é˜…è¯»åŸæ–‡]({article['link']})\n\n"
        
        # 2. å›½é™…æ–°é—»åŒºï¼ˆè‹±æ–‡ï¼Œæ˜¾ç¤ºç¿»è¯‘+åŸæ–‡ï¼‰
        if intl_articles:
            section += f"\n### ğŸŒ å›½é™…æ–°é—»\n\n"
            for i, article in enumerate(intl_articles, 1):
                emoji = "â­ï¸" if article.get('importance', 0) > 7 else "ğŸ“"
                
                # ä¼˜å…ˆæ˜¾ç¤ºç¿»è¯‘åçš„æ ‡é¢˜
                title_cn = article.get('title_translated', article['title'])
                title_en = article['title']
                source = article['source']
                
                # æ ¼å¼ï¼šç¿»è¯‘æ ‡é¢˜ (Original: è‹±æ–‡åŸæ–‡)
                section += f"{i}. {emoji} **{title_cn}**"
                if 'title_translated' in article and title_cn != title_en:
                    section += f" (Original: {title_en})"
                section += "\n"
                
                section += f"   ğŸ“ {source}"
                
                # æ·»åŠ äº’åŠ¨æ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
                if article.get('points', 0) > 0:
                    section += f" | ğŸ‘ {article['points']}"
                if article.get('comments', 0) > 0:
                    section += f" | ğŸ’¬ {article['comments']}"
                
                section += f"\n   ğŸ”— [é˜…è¯»åŸæ–‡]({article['link']})\n\n"
        
        # æ·»åŠ ç²¾é€‰äº‹å®æ–°é—»
        if self.featured_fact:
            featured_title = self.featured_fact.get('title_translated', self.featured_fact['title'])
            featured_summary = self.featured_fact.get('summary_translated', self.featured_fact.get('summary', 'ç‚¹å‡»é“¾æ¥æŸ¥çœ‹è¯¦æƒ…'))
            orig_title = self.featured_fact['title'] if 'title_translated' in self.featured_fact else ''
            orig_summary = self.featured_fact.get('summary', '') if 'summary_translated' in self.featured_fact else ''
            
            orig_title_part = "(Original: " + orig_title + ")" if orig_title else ""
            orig_summary_part = "\n\nOriginal Summary: " + orig_summary if orig_summary else ""
            
            section += f"""
## ğŸ“° ä»Šæ—¥äº‹å®ç²¾é€‰

**{featured_title}** {orig_title_part}

**æ¥æº**: {self.featured_fact['source']} | **æ—¶é—´**: {self.featured_fact.get('time', 'ä»Šæ—¥')}

**æ‘˜è¦**: {featured_summary}{orig_summary_part}

**ğŸ”— æ·±åº¦é˜…è¯»**: {self.featured_fact['link']}
"""
        
        section += f"""
---
*äº‹å®æ–°é—»æ¥è‡ª {len(set([a['source'] for a in self.fact_articles]))} ä¸ªå›½å†…å¤–æƒå¨åª’ä½“*
*æ¯æ—¥ç­›é€‰è¿‡å»48å°æ—¶æœ€é‡è¦æ–°é—»ï¼Œä¿æŒä¿¡æ¯å¹¿åº¦ä¸æ·±åº¦*
"""
        
        return section
    
    def generate_report(self):
        """ç”Ÿæˆå®Œæ•´æŠ¥å‘Š"""
        current_time = datetime.now().strftime('%Y-%m-%d %H:%M')
        
        report = f"""# ğŸ“Š æ¯æ—¥èµ„è®¯åŒæŠ¥å‘Š ({current_time})

## ğŸ“ˆ æ•°æ®æ€»è§ˆ
- **AIç§‘æŠ€èµ„è®¯**: {len(self.ai_articles)} ç¯‡
- **äº‹å®èµ„è®¯**: {len(self.fact_articles)} ç¯‡
- **æ·±åº¦åˆ†æ**: {len(self.deep_analyses)} ç¯‡
- **è¦†ç›–åª’ä½“**: {len(self.ai_news_sources) + len(self.fact_news_sources)} ä¸ª

"""
        
        # 1. AIç§‘æŠ€æ–°é—»éƒ¨åˆ†
        if self.ai_articles:
            report += f"""
## ğŸ¤– AIç§‘æŠ€æ—¥æŠ¥

### ğŸš€ AIå¿«è®¯æ‘˜è¦
"""
            # æŒ‰ç±»åˆ«åˆ†ç»„å±•ç¤ºAIæ–°é—»
            ai_by_category = {}
            for article in self.ai_articles[:15]:
                cat = article.get('category', 'other')
                if cat not in ai_by_category:
                    ai_by_category[cat] = []
                ai_by_category[cat].append(article)
            
            category_names = {
                'research': 'ğŸ§ª ç ”ç©¶å‰æ²¿',
                'tech': 'ğŸ”§ æŠ€æœ¯åŠ¨æ€',
                'community': 'ğŸ‘¥ ç¤¾åŒºçƒ­ç‚¹',
                'cn_ai': 'ğŸ‡¨ğŸ‡³ å›½å†…AI'
            }
            
            for cat, articles in ai_by_category.items():
                name = category_names.get(cat, 'ğŸ“Œ å…¶ä»–')
                report += f"\n**{name}**\n"
                for i, article in enumerate(articles[:3], 1):
                    title_display = article.get('title_translated', article['title'])
                    report += f"{i}. {title_display}\n"
                    report += f"   ğŸ“ {article['source']} | ğŸ”— [é˜…è¯»åŸæ–‡]({article['link']})\n"
            
            # AIæ·±åº¦åˆ†æ
            if self.deep_analyses:
                report += "\n## ğŸ” AIæ·±åº¦åˆ†æ\n"
                report += "_ä»¥ä¸‹AIæ–‡ç« å·²è¿›è¡Œè¯¦ç»†æŠ€æœ¯åˆ†æï¼š_\n\n"
                for analysis in self.deep_analyses:
                    report += analysis['text']
            
            # AIç²¾é€‰
            if self.featured_article:
                featured_title = self.featured_article.get('title_translated', self.featured_article['title'])
                featured_summary = self.featured_article.get('summary_translated', self.featured_article.get('summary', 'æš‚æ— æ‘˜è¦'))
                
                report += f"""
## ğŸ† ä»Šæ—¥AIç²¾é€‰

**{featured_title}**

**æ¥æº**: {self.featured_article['source']}
**æ‘˜è¦**: {featured_summary}

**ğŸ”— æ·±åº¦é˜…è¯»**: {self.featured_article['link']}
"""
        
        # 2. äº‹å®æ–°é—»éƒ¨åˆ†
        report += self.format_fact_news_section()
        
        # 3. æ€»ç»“
        report += f"""

---

## ğŸ“‹ æŠ¥å‘Šä¿¡æ¯
- **ç”Ÿæˆæ—¶é—´**: {current_time}
- **ä¸‹æ¬¡æ›´æ–°**: æ˜æ—¥ 08:00 (åŒ—äº¬æ—¶é—´)
- **åˆ†ææ”¯æŒ**: Gemini
- **æ¨é€æ–¹å¼**: Serveré…±å¾®ä¿¡æ¨é€

*ä¿æŒä¿¡æ¯æ•æ„Ÿåº¦ï¼Œæ‹¥æŠ±ç§‘æŠ€å˜é©ï¼Œå…³æ³¨ä¸–ç•ŒåŠ¨æ€*
"""
        
        title = f"èµ„è®¯åŒæŠ¥å‘Š {datetime.now().strftime('%m-%d')} | AI:{len(self.ai_articles)} äº‹å®:{len(self.fact_articles)}"
        
        return report, title
    
    def save_reports(self, report):
        """ä¿å­˜æŠ¥å‘Š"""
        output_data = {
            'fetch_time': datetime.now().isoformat(),
            'ai_articles_count': len(self.ai_articles),
            'fact_articles_count': len(self.fact_articles),
            'deep_analyses_count': len(self.deep_analyses),
            'featured_article': self.featured_article,
            'featured_fact': self.featured_fact,
            'ai_articles': self.ai_articles[:20],
            'fact_articles': self.fact_articles[:10]
        }
        
        with open('enhanced_news_analysis.json', 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        with open('enhanced_news_report.md', 'w', encoding='utf-8') as f:
            f.write(report)
        
        print("ğŸ’¾ æŠ¥å‘Šå·²ä¿å­˜è‡³: enhanced_news_analysis.json, enhanced_news_report.md")
    
    def send_to_wechat(self, report):
        """é€šè¿‡Serveré…±å‘é€åˆ°å¾®ä¿¡"""
        if not self.server_chan_key:
            print("âš ï¸ æœªé…ç½®Serveré…±å¯†é’¥ï¼Œè·³è¿‡æ¨é€")
            return False
        
        url = f"https://sctapi.ftqq.com/{self.server_chan_key}.send"
        
        if len(report) > 20000:
            report = report[:20000] + "\n\n...ï¼ˆæŠ¥å‘Šè¿‡é•¿ï¼Œå·²æˆªæ–­ï¼‰"
        
        data = {
            'title': f"èµ„è®¯åŒæŠ¥å‘Š {datetime.now().strftime('%m-%d')} | AI:{len(self.ai_articles)} äº‹å®:{len(self.fact_articles)}",
            'desp': report
        }
        
        try:
            response = requests.post(url, data=data, timeout=15)
            result = response.json()
            
            if result.get('code') == 0:
                print(f"âœ… å¾®ä¿¡æ¨é€æˆåŠŸï¼æ¶ˆæ¯ID: {result.get('data', {}).get('pushid')}")
                return True
            else:
                print(f"âŒ æ¨é€å¤±è´¥: {result}")
                return False
        except Exception as e:
            print(f"âŒ æ¨é€è¯·æ±‚å¤±è´¥: {e}")
            return False
    
    async def run_async(self):
        """å¼‚æ­¥ä¸»æ‰§è¡Œå‡½æ•°ï¼ˆå¸¦å¼‚å¸¸å¤„ç†ï¼‰"""
        if not ASYNC_AVAILABLE:
            raise RuntimeError("å¼‚æ­¥åŠŸèƒ½ä¸å¯ç”¨ï¼šè¯·å…ˆå®‰è£… aiohttp (pip install aiohttp)")
        
        print("=" * 70)
        print("ğŸ“Š å¢å¼ºç‰ˆèµ„è®¯åˆ†æç³»ç»Ÿå¯åŠ¨ (å¼‚æ­¥æ¨¡å¼)")
        print(f"ğŸ“… æ‰§è¡Œæ—¶é—´: {datetime.now()}")
        print("=" * 70)
        
        try:
            # 1. å¼‚æ­¥æŠ“å–AIæ–°é—»
            await self.fetch_all_news_async()
            
            # 2. å¼‚æ­¥æŠ“å–äº‹å®æ–°é—»
            await self.fetch_fact_news_async()
            
            if not self.all_articles:
                print("âŒ æœªæŠ“å–åˆ°ä»»ä½•æ–‡ç« ï¼Œç¨‹åºé€€å‡º")
                return self._generate_error_report("æœªæŠ“å–åˆ°ä»»ä½•æ–°é—»æ–‡ç« "), "æŠ“å–å¤±è´¥"
            
            # 3. ç”ŸæˆAIæ·±åº¦åˆ†æ
            self.generate_deep_analyses(limit=3)
            
            # 4. é€‰æ‹©ç²¾é€‰æ–‡ç« 
            self.select_featured_articles()
            
            # 5. ç”ŸæˆæŠ¥å‘Š
            report, title = self.generate_report()
            
            # 6. ä¿å­˜æŠ¥å‘Š
            self.save_reports(report)
            
            print(f"\nğŸ“Š æŠ¥å‘Šç”Ÿæˆå®Œæˆ:")
            print(f"   AIèµ„è®¯: {len(self.ai_articles)} ç¯‡")
            print(f"   äº‹å®èµ„è®¯: {len(self.fact_articles)} ç¯‡")
            print(f"   æŠ¥å‘Šæ ‡é¢˜: {title}")
            
            return report, title
            
        except KeyboardInterrupt:
            print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
            return self._generate_error_report("ç”¨æˆ·æ‰‹åŠ¨ä¸­æ–­æ‰§è¡Œ"), "æ‰§è¡Œä¸­æ–­"
            
        except Exception as e:
            print(f"\nâŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            print(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{traceback.format_exc()}")
            return self._generate_error_report(f"æ‰§è¡Œå¼‚å¸¸: {str(e)}"), "æ‰§è¡Œå¤±è´¥"
    
    def run(self):
        """ä¸»æ‰§è¡Œå‡½æ•°ï¼ˆå¸¦å¼‚å¸¸å¤„ç†ï¼‰"""
        print("=" * 70)
        print("ğŸ“Š å¢å¼ºç‰ˆèµ„è®¯åˆ†æç³»ç»Ÿå¯åŠ¨")
        print(f"ğŸ“… æ‰§è¡Œæ—¶é—´: {datetime.now()}")
        print("=" * 70)
        
        try:
            # 1. æŠ“å–AIæ–°é—»
            self.fetch_all_news()
            
            # 2. æŠ“å–äº‹å®æ–°é—»
            self.fetch_fact_news()
            
            if not self.all_articles:
                print("âŒ æœªæŠ“å–åˆ°ä»»ä½•æ–‡ç« ï¼Œç¨‹åºé€€å‡º")
                return self._generate_error_report("æœªæŠ“å–åˆ°ä»»ä½•æ–°é—»æ–‡ç« "), "æŠ“å–å¤±è´¥"
            
            # 3. ç”ŸæˆAIæ·±åº¦åˆ†æ
            self.generate_deep_analyses(limit=3)
            
            # 4. é€‰æ‹©ç²¾é€‰æ–‡ç« 
            self.select_featured_articles()
            
            # 5. ç”ŸæˆæŠ¥å‘Š
            report, title = self.generate_report()
            
            # 6. ä¿å­˜æŠ¥å‘Š
            self.save_reports(report)
            
            print(f"\nğŸ“Š æŠ¥å‘Šç”Ÿæˆå®Œæˆ:")
            print(f"   AIèµ„è®¯: {len(self.ai_articles)} ç¯‡")
            print(f"   äº‹å®èµ„è®¯: {len(self.fact_articles)} ç¯‡")
            print(f"   æŠ¥å‘Šæ ‡é¢˜: {title}")
            
            return report, title
            
        except KeyboardInterrupt:
            print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
            return self._generate_error_report("ç”¨æˆ·æ‰‹åŠ¨ä¸­æ–­æ‰§è¡Œ"), "æ‰§è¡Œä¸­æ–­"
            
        except Exception as e:
            print(f"\nâŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            print(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{traceback.format_exc()}")
            return self._generate_error_report(f"æ‰§è¡Œå¼‚å¸¸: {str(e)}"), "æ‰§è¡Œå¤±è´¥"
    
    def _generate_error_report(self, error_message):
        """ç”Ÿæˆé”™è¯¯æƒ…å†µä¸‹çš„ç®€åŒ–æŠ¥å‘Š"""
        current_time = datetime.now().strftime('%Y-%m-%d %H:%M')
        
        error_report = f"""# ğŸ“Š èµ„è®¯åˆ†ææŠ¥å‘Š (æ‰§è¡Œå¤±è´¥)

## âš ï¸ æ‰§è¡ŒçŠ¶æ€
- **çŠ¶æ€**: æ‰§è¡Œå¤±è´¥
- **é”™è¯¯ä¿¡æ¯**: {error_message}
- **æ‰§è¡Œæ—¶é—´**: {current_time}

## ğŸ“ˆ å½“å‰æ•°æ®ç»Ÿè®¡
- **AIç§‘æŠ€èµ„è®¯**: {len(self.ai_articles)} ç¯‡
- **äº‹å®èµ„è®¯**: {len(self.fact_articles)} ç¯‡
- **æ·±åº¦åˆ†æ**: {len(self.deep_analyses)} ç¯‡

## ğŸ“‹ å·²è·å–å†…å®¹é¢„è§ˆ
"""
        
        # æ·»åŠ å·²æˆåŠŸæŠ“å–çš„æ–‡ç« é¢„è§ˆ
        if self.ai_articles:
            error_report += f"\n### ğŸ¤– å·²è·å–çš„AIèµ„è®¯ ({len(self.ai_articles)}ç¯‡)\n"
            for i, article in enumerate(self.ai_articles[:5], 1):  # æœ€å¤šæ˜¾ç¤º5ç¯‡
                title_display = article.get('title_translated', article['title'])
                error_report += f"{i}. {title_display}\n"
                error_report += f"   ğŸ“ {article['source']} | ğŸ”— [åŸæ–‡é“¾æ¥]({article['link']})\n\n"
        
        if self.fact_articles:
            error_report += f"\n### ğŸŒ å·²è·å–çš„äº‹å®èµ„è®¯ ({len(self.fact_articles)}ç¯‡)\n"
            for i, article in enumerate(self.fact_articles[:5], 1):  # æœ€å¤šæ˜¾ç¤º5ç¯‡
                title_display = article.get('title_translated', article['title'])
                error_report += f"{i}. {title_display}\n"
                error_report += f"   ğŸ“ {article['source']} | ğŸ”— [åŸæ–‡é“¾æ¥]({article['link']})\n\n"
        
        error_report += f"""
---

## ğŸ› ï¸ å»ºè®®è§£å†³æ–¹æ¡ˆ
1. æ£€æŸ¥ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸
2. ç¡®è®¤å„æ–°é—»æºæ˜¯å¦å¯è®¿é—®
3. éªŒè¯APIå¯†é’¥é…ç½®æ˜¯å¦æ­£ç¡®
4. æŸ¥çœ‹è¯¦ç»†é”™è¯¯æ—¥å¿—å®šä½é—®é¢˜

*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {current_time}*
"""
        
        return error_report

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='AIç§‘æŠ€èµ„è®¯åˆ†æç³»ç»Ÿ')
    parser.add_argument('--use-async', action='store_true', help='ä½¿ç”¨å¼‚æ­¥æ¨¡å¼åŠ é€ŸæŠ“å–')
    args = parser.parse_args()
    
    analyzer = EnhancedNewsAnalyzer()
    
    if args.use_async:
        # å¼‚æ­¥æ¨¡å¼
        if not ASYNC_AVAILABLE:
            print("âŒ å¼‚æ­¥æ¨¡å¼ä¸å¯ç”¨ï¼šè¯·å…ˆå®‰è£… aiohttp")
            print("   å®‰è£…å‘½ä»¤: pip install aiohttp")
            print("   æˆ–ä½¿ç”¨åŒæ­¥æ¨¡å¼: python tech_news_ai_with_facts.py")
            return
        
        print("ğŸš€ å¯åŠ¨å¼‚æ­¥æ¨¡å¼...")
        report, title = asyncio.run(analyzer.run_async())
    else:
        # åŒæ­¥æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
        report, title = analyzer.run()
    
    if report:
        if analyzer.server_chan_key:
            print("\nğŸ“¤ æ­£åœ¨å‘é€åˆ°å¾®ä¿¡...")
            analyzer.send_to_wechat(report)
        else:
            print("\nâš ï¸ æœªé…ç½®SERVER_CHAN_KEYï¼Œè·³è¿‡æ¨é€")
        
        # æ‰“å°é¢„è§ˆ
        print("\n" + "=" * 70)
        print("ğŸ“‹ å†…å®¹é¢„è§ˆ:")
        print("=" * 70)
        preview_length = min(2000, len(report))
        print(report[:preview_length] + "..." if len(report) > preview_length else report)
    else:
        print("âŒ æœªç”ŸæˆæŠ¥å‘Šï¼Œè¯·æ£€æŸ¥é…ç½®")

if __name__ == "__main__":
    main()
